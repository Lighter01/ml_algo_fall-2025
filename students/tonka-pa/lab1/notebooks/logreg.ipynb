{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d24d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccdebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9\n",
    "\n",
    "SEED = 18092025\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "rng = np.random.default_rng(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1348d94a",
   "metadata": {},
   "source": [
    "[Датасет](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('../datasets/data.csv', delimiter=';')\n",
    "except Exception:\n",
    "    print('No such file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.max_colwidth')\n",
    "# pd.reset_option('display.width')\n",
    "# pd.reset_option('display.expand_frame_repr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628b879",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb670c",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bbea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nans(df):\n",
    "    nans_per_col = [(col, df[col].isna().sum(), df[col].isna().sum() / df.shape[0] * 100) for col in df.columns]\n",
    "    dtype = [('col_name', 'U20'), ('nans', int), ('nans_perc', float)]\n",
    "    nans_per_col = np.array(nans_per_col, dtype=dtype)\n",
    "    nans_per_col = nans_per_col[nans_per_col['nans'] > 0]\n",
    "    nans_per_col = np.sort(nans_per_col, order='nans')\n",
    "\n",
    "    if nans_per_col.shape[0] == 0:\n",
    "        print('No nans in the dataset')\n",
    "        return\n",
    "\n",
    "    df_show = pd.DataFrame(nans_per_col[::-1])\n",
    "    display(df_show.style.background_gradient(cmap='Blues'))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "    y_pos = np.arange(len(nans_per_col))\n",
    "    \n",
    "    ax.barh(y_pos, nans_per_col['nans_perc'], alpha=0.8, edgecolor='black', linewidth=1) \n",
    "    ax.set_yticks(y_pos, labels=nans_per_col['col_name'])\n",
    "    ax.set_xlabel('Nans, %', fontsize=14)\n",
    "    ax.set_title('Nans rate for each column', fontsize=16)\n",
    "    ax.set_xlim(0, min(np.max(df_show['nans_perc']) + 5.0, 100.0))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "    ax.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "print('Dataset shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef577992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_names_transform(col_name: str) -> str:\n",
    "    res_name = col_name.strip().replace(\"\\t\", \"\").replace(' ', '_').lower()\n",
    "    return res_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = map(col_names_transform, df.columns.values)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07cb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: добавить другие стат. показатели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36286643",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_nans(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ddf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts(normalize=True).to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0ef30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566fc32",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e69653",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=['target']), df['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7df02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, shuffle=True, stratify=y)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_train_scaled = std_scaler.fit_transform(X_train)\n",
    "X_test_scaled  = std_scaler.transform(X_test)\n",
    "X_train_scaled[0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train.values)\n",
    "y_test_enc  = label_encoder.transform(y_test.values)\n",
    "y_train[:5].values, y_train_enc[:5], label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7099c0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6d34f",
   "metadata": {},
   "source": [
    "## Модель линейной классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    def __init__(self):\n",
    "        self.value = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, new_value):\n",
    "        self.count += 1\n",
    "        self.value = (1 / self.count) * new_value + (1 - 1 / self.count) * self.value\n",
    "        return self.value\n",
    "    \n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, l=0.1):\n",
    "        self.value = None\n",
    "        self.l = l\n",
    "\n",
    "    def update(self, new_value):\n",
    "        if self.value is not None:\n",
    "            self.value = self.l * new_value + (1 - self.l) * self.value\n",
    "        else:\n",
    "            self.value = new_value\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegNumpy():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_weights: list[list[float]] = None, # (n_features, n_classes)\n",
    "        initial_bias:    list[float] = None,       # (1, n_classes)\n",
    "        tolerance:       float = 1e-4,\n",
    "        early_stop: bool = False,\n",
    "        n_startup_rounds: int = 50,\n",
    "        early_stop_rounds: int = 50,\n",
    "        random_seed: int = SEED\n",
    "    ):\n",
    "        self.weights = (np.array(initial_weights) if initial_weights is not None \n",
    "                        else np.array([]))\n",
    "        self.bias = (np.array(initial_bias) if initial_bias is not None \n",
    "                     else np.array([]))\n",
    "        self.tolerance = tolerance\n",
    "        self.early_stop = early_stop\n",
    "        self.n_startup_rounds = n_startup_rounds\n",
    "        self.early_stop_rounds = early_stop_rounds\n",
    "        self.random_seed = random_seed\n",
    "        self.eps = 1e-9\n",
    "\n",
    "        # для рекуррентной оценки\n",
    "        self.rec_value = None\n",
    "        self.rec_count = 0\n",
    "        self.rec_history = []\n",
    "\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        features:       list[list[float]],\n",
    "        labels:         list[int],\n",
    "        learning_rate:  float = 1e-3,\n",
    "        epochs:         int = 100,\n",
    "        shuffle:        bool = True,\n",
    "        training_mode:  str = 'per_sample', # \"full\" | \"batch\"\n",
    "        momentum:       float = 0.0,\n",
    "        l2:             float = 0.0,\n",
    "        return_weights_history:  bool = False,\n",
    "        verbose:                 bool = False,\n",
    "        rec_mode:                str = 'off',  # \"off\" | \"mean\" | \"ema\"\n",
    "        ema_lambda:              float = 0.1,  # λ для EMA\n",
    "        early_stop_use_rec:      bool = True,  # ранняя остановка по рекурсивной оценке\n",
    "    ) -> None | list[list[float]]:\n",
    "        X = (np.array(features).squeeze() if not isinstance(features, np.ndarray) \n",
    "             else deepcopy(features).astype(np.float32, copy=False))\n",
    "        y = (np.array(labels).squeeze() if not isinstance(labels, np.ndarray) \n",
    "             else deepcopy(labels).astype(np.int32, copy=False))\n",
    "\n",
    "        self._init_weights(X, y)\n",
    "        if return_weights_history:\n",
    "            weights_values = [self.weights.copy()]\n",
    "        # velocities for momentum\n",
    "        Vdw = np.zeros_like(self.weights)\n",
    "        Vdb = np.zeros_like(self.bias)\n",
    "            \n",
    "        self._rec_reset()\n",
    "        self.loss_values = []\n",
    "        no_improvement_counter = 0\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for epoch in range(0, epochs+1):\n",
    "\n",
    "            if training_mode == 'per_sample':\n",
    "                idx = np.arange(N)\n",
    "                if shuffle:\n",
    "                    rng.shuffle(idx)\n",
    "                \n",
    "                epoch_loss_sum = 0.0\n",
    "\n",
    "                for i in idx:\n",
    "                    xi = X[i:i+1, :] # (1, n_features)\n",
    "                    yi = y[i, np.newaxis] # (1,)\n",
    "                    logits_i = self.forward(xi)\n",
    "                    loss_i   = self._loss_fn_opt(yi, logits_i)\n",
    "                    \n",
    "                    epoch_loss_sum += loss_i[0]\n",
    "\n",
    "                    rec_val = self._rec_update(loss_i[0], mode=rec_mode, ema_lambda=ema_lambda)\n",
    "                    self.rec_history.append(rec_val)\n",
    "\n",
    "                    w_grad, b_grad = self._gradient(xi, yi, logits_i)\n",
    "\n",
    "                    # L2\n",
    "                    if l2 > 0.0:\n",
    "                        w_grad += l2 * self.weights\n",
    "                    \n",
    "                    # momentum\n",
    "                    Vdw = momentum * Vdw + (1 - momentum) * w_grad\n",
    "                    Vdb = momentum * Vdb + (1 - momentum) * b_grad\n",
    "\n",
    "                    # GD step\n",
    "                    self.weights -= learning_rate * Vdw\n",
    "                    self.bias    -= learning_rate * Vdb\n",
    "\n",
    "                mean_epoch_loss = epoch_loss_sum / N\n",
    "                monitored = self.rec_history[-1] if (rec_mode != 'off' and early_stop_use_rec) else mean_epoch_loss\n",
    "                self.loss_values.append(monitored)\n",
    "\n",
    "                if verbose and (epoch % max(1, epochs // 10) == 0):\n",
    "                    print(f\"epoch {epoch:4d} | loss={monitored:.6f}\")\n",
    "            \n",
    "            elif training_mode == 'full':\n",
    "\n",
    "                logits = self.forward(X)\n",
    "                loss = self._loss_fn_opt(y, logits, reduction='mean')\n",
    "                self.loss_values.append(loss)\n",
    "\n",
    "                if return_weights_history:\n",
    "                    weights_values.append(self.weights.copy())\n",
    "                \n",
    "                w_grad, b_grad = self._gradient(X, y, logits)\n",
    "                \n",
    "                # l2 regularization\n",
    "                if l2 > 0.0:\n",
    "                    w_grad += l2 * self.weights\n",
    "\n",
    "                # momentum\n",
    "                Vdw = momentum * Vdw + (1 - momentum) * w_grad\n",
    "                Vdb = momentum * Vdb + (1 - momentum) * b_grad\n",
    "\n",
    "                # gradient descent step\n",
    "                self.weights -= learning_rate * Vdw\n",
    "                self.bias -= learning_rate * Vdb\n",
    "\n",
    "                if verbose and (epoch % max(1, epochs // 10) == 0):\n",
    "                    print(f\"epoch {epoch:4d} | loss={loss:.6f}\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"training_mode must be 'full' or 'per_sample'.\")\n",
    "            \n",
    "            if self.early_stop and epoch > self.n_startup_rounds and len(self.loss_values) > 1:\n",
    "                if 0 < (self.loss_values[-2] - self.loss_values[-1]) < self.tolerance:\n",
    "                    no_improvement_counter += 1\n",
    "                    if no_improvement_counter >= self.early_stop_rounds:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "                else:\n",
    "                    no_improvement_counter = 0\n",
    "\n",
    "        if return_weights_history:\n",
    "            return np.array(weights_values)\n",
    "        \n",
    "\n",
    "    def predict(self, features: list[list[float]]):\n",
    "        X = (np.array(features).squeeze() if not isinstance(features, np.ndarray) \n",
    "             else deepcopy(features).astype(np.float32, copy=False))\n",
    "        if X.ndim == 1:\n",
    "            X = X[np.newaxis, :]\n",
    "        logits = self.forward(X) # (n_samples, n_classes)\n",
    "        probs  = self._softmax(logits) # не обязательно\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def _create_onehot_target(self, y: np.array):\n",
    "        ohe_enc = OneHotEncoder(categories=[np.unique(y)], sparse_output=False)\n",
    "        y_enc = ohe_enc.fit_transform(y.reshape(-1, 1))\n",
    "        return y_enc # output -> (n_samples, n_classes)\n",
    "    \n",
    "    def _init_weights(self, X, y):\n",
    "        rng_ = np.random.default_rng(seed=self.random_seed)\n",
    "        n_unique_classes = np.unique(y).shape[0]\n",
    "        if self.weights.size == 0:\n",
    "            self.weights = rng_.standard_normal((X.shape[1], n_unique_classes), dtype=np.float32)\n",
    "        if self.bias.size == 0:\n",
    "            self.bias = rng_.standard_normal((1, n_unique_classes), dtype=np.float32)\n",
    "        \n",
    "    def _softmax(self, X: np.array) -> np.array:\n",
    "        Z = X - np.max(X, axis=1, keepdims=True)\n",
    "        numerator = np.exp(Z)\n",
    "        denominator = np.sum(numerator, axis=1, keepdims=True)\n",
    "        softmax_probs = numerator / denominator\n",
    "        return softmax_probs # -> (n_samples, n_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # (n_samples, n_features) * (n_features, n_classes)\n",
    "        logits = np.matmul(X, self.weights) + self.bias # -> (n_samples, n_classes)\n",
    "        return logits\n",
    "    \n",
    "    # def loss_fn_expanded(self, X, y_true):\n",
    "    #     # (n_samples, n_features) * (n_features, n_classes) + (n_samples, 1) * (1, n_classes) = (n_samples, n_classes)\n",
    "    #     logits = np.matmul(X, self.weights) + np.matmul(np.ones((X.shape[0], 1)), self.bias)\n",
    "    #     exp_logits = np.exp(logits)\n",
    "    #     logits_sum = np.sum(exp_logits, axis=1) # -> (n_samples, 1)\n",
    "    #     # (n_samples, n_classes) * (n_samples, n_classes)\n",
    "    #     true_class_logits = logits[np.arange(X.shape[0]), y_true]\n",
    "    #     return np.mean(np.log(logits_sum) - true_class_logits)\n",
    "\n",
    "    # def loss_fn(self, y_true, logits):\n",
    "    #     log_probs = np.log(self.softmax(logits)) # -> (n_samples, classes)\n",
    "    #     # y_true_ohe = self.create_onehot_target(y_true) # -> (n_samples, classes)\n",
    "    #     # likelihood = (log_probs * y_true_ohe).sum(axis=1).mean()\n",
    "    #     likelihood = (log_probs[np.arange(log_probs.shape[0]), y_true]).mean()\n",
    "    #     return -likelihood\n",
    "    \n",
    "    def _loss_fn_opt(self, y_true, logits, reduction=None):\n",
    "        lse = logsumexp(logits, axis=1, keepdims=True)\n",
    "        nll = lse - logits\n",
    "        loss = nll[np.arange(nll.shape[0]), y_true]\n",
    "        if reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "    \n",
    "    def _rec_reset(self):\n",
    "        self.rec_value = None\n",
    "        self.rec_count = 0\n",
    "        self.rec_history = []\n",
    "\n",
    "    def _rec_update(self, xi, mode=\"off\", ema_lambda=0.1):\n",
    "        if mode == \"off\":\n",
    "            return xi\n",
    "\n",
    "        if self.rec_value is None:\n",
    "            # инициализация последовательности\n",
    "            self.rec_value = xi\n",
    "            self.rec_count = 1\n",
    "            return self.rec_value\n",
    "\n",
    "        if mode == \"mean\":\n",
    "            # running mean: Q_m = (1/m)*xi_m + (1 - 1/m)*Q_{m-1}\n",
    "            self.rec_count += 1\n",
    "            m = self.rec_count\n",
    "            self.rec_value = (1.0/m)*xi + (1.0 - 1.0/m)*self.rec_value\n",
    "            return self.rec_value\n",
    "\n",
    "        if mode == \"ema\":\n",
    "            # EMA: Q_m = λ xi_m + (1 - λ) Q_{m-1}\n",
    "            self.rec_value = ema_lambda * xi + (1.0 - ema_lambda) * self.rec_value\n",
    "            return self.rec_value\n",
    "\n",
    "        return xi\n",
    "\n",
    "    def _gradient(self, X, y_true, logits):\n",
    "        y_prob = self._softmax(logits)\n",
    "        y_prob[np.arange(y_prob.shape[0]), y_true] -= 1\n",
    "        y_prob /= y_prob.shape[0]\n",
    "        w_grad = np.matmul(X.T, y_prob)\n",
    "        b_grad = y_prob.sum(axis=0, keepdims=True)\n",
    "        return w_grad, b_grad\n",
    "    \n",
    "\n",
    "    def calc_margins(self, X, y_true, plot: bool = False, eps=1e-3, **kwargs):\n",
    "        logits = self.forward(X)\n",
    "        true_logits = logits[np.arange(X.shape[0]), y_true]\n",
    "        logits[np.arange(logits.shape[0]), y_true] = -np.inf\n",
    "        false_logits = logits.max(axis=1)\n",
    "        margins = true_logits - false_logits\n",
    "\n",
    "        if plot:\n",
    "            \n",
    "            sorted_idx = np.argsort(margins)\n",
    "            sorted_margins = margins[sorted_idx]\n",
    "            \n",
    "            line_kwargs      = {'lw': 2}\n",
    "            pos_fill_kwargs  = {'alpha': 0.25, 'color': 'tab:green'}\n",
    "            neg_fill_kwargs  = {'alpha': 0.25, 'color': 'tab:red'}\n",
    "            zero_fill_kwargs = {'alpha': 0.25, 'color': 'gold'}\n",
    "\n",
    "            # masks\n",
    "            if eps > 0.0:\n",
    "                mask_zero = np.abs(sorted_margins) <= eps\n",
    "                mask_pos  = sorted_margins >  eps\n",
    "                mask_neg  = sorted_margins < -eps\n",
    "            else:\n",
    "                mask_zero = np.zeros_like(sorted_margins, dtype=bool)\n",
    "                mask_pos  = sorted_margins > 0\n",
    "                mask_neg  = sorted_margins < 0\n",
    "\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            # line\n",
    "            plot_idx = np.arange(sorted_margins.shape[0])\n",
    "            plt.plot(plot_idx, sorted_margins, **line_kwargs)\n",
    "            plt.axhline(0.0, color='black', lw=1, alpha=0.7)\n",
    "\n",
    "            if np.any(mask_neg):\n",
    "                plt.fill_between(plot_idx, sorted_margins, 0.0, where=mask_neg, interpolate=True, **neg_fill_kwargs)\n",
    "            if np.any(mask_zero):\n",
    "                plt.fill_between(plot_idx, sorted_margins, 0.0, where=mask_zero, interpolate=True, **zero_fill_kwargs)\n",
    "            if np.any(mask_pos):\n",
    "                plt.fill_between(plot_idx, sorted_margins, 0.0, where=mask_pos, interpolate=True, **pos_fill_kwargs)\n",
    "\n",
    "            plt.xlabel(\"sample index (sorted)\")\n",
    "            plt.ylabel(\"margin\")\n",
    "            plt.title(\"Margin curve with signed areas\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        return margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogRegNumpy()\n",
    "logreg._init_weights(X_train_scaled, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(\n",
    "    X_train_scaled, y_train_enc, \n",
    "    learning_rate=0.01, epochs=10,\n",
    "    momentum=0.99, l2=0.001,\n",
    "    shuffle=True, training_mode='per_sample',\n",
    "    verbose=True, rec_mode='ema', early_stop_use_rec=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfaad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logreg.loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logreg.rec_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.calc_margins(X_train_scaled, y_train_enc, plot=True, eps=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aacea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl python 3.11.5",
   "language": "python",
   "name": "wsl_main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
