{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d24d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccdebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9\n",
    "\n",
    "SEED = 18092025\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1348d94a",
   "metadata": {},
   "source": [
    "[Датасет](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('../datasets/data.csv', delimiter=';')\n",
    "except Exception:\n",
    "    print('No such file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.max_colwidth')\n",
    "# pd.reset_option('display.width')\n",
    "# pd.reset_option('display.expand_frame_repr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628b879",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb670c",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bbea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nans(df):\n",
    "    nans_per_col = [(col, df[col].isna().sum(), df[col].isna().sum() / df.shape[0] * 100) for col in df.columns]\n",
    "    dtype = [('col_name', 'U20'), ('nans', int), ('nans_perc', float)]\n",
    "    nans_per_col = np.array(nans_per_col, dtype=dtype)\n",
    "    nans_per_col = nans_per_col[nans_per_col['nans'] > 0]\n",
    "    nans_per_col = np.sort(nans_per_col, order='nans')\n",
    "\n",
    "    if nans_per_col.shape[0] == 0:\n",
    "        print('No nans in the dataset')\n",
    "        return\n",
    "\n",
    "    df_show = pd.DataFrame(nans_per_col[::-1])\n",
    "    display(df_show.style.background_gradient(cmap='Blues'))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "    y_pos = np.arange(len(nans_per_col))\n",
    "    \n",
    "    ax.barh(y_pos, nans_per_col['nans_perc'], alpha=0.8, edgecolor='black', linewidth=1) \n",
    "    ax.set_yticks(y_pos, labels=nans_per_col['col_name'])\n",
    "    ax.set_xlabel('Nans, %', fontsize=14)\n",
    "    ax.set_title('Nans rate for each column', fontsize=16)\n",
    "    ax.set_xlim(0, min(np.max(df_show['nans_perc']) + 5.0, 100.0))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "    ax.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "print('Dataset shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef577992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_names_transform(col_name: str) -> str:\n",
    "    res_name = col_name.strip().replace(\"\\t\", \"\").replace(' ', '_').lower()\n",
    "    return res_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = map(col_names_transform, df.columns.values)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07cb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: добавить другие стат. показатели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36286643",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_nans(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ddf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts(normalize=True).to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0ef30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566fc32",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e69653",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=['target']), df['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7df02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=SEED, shuffle=True, stratify=y)\n",
    "X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED, shuffle=True, stratify=y_train)\n",
    "y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_train_scaled = std_scaler.fit_transform(X_train)\n",
    "X_val_scaled   = std_scaler.transform(X_val)\n",
    "X_test_scaled  = std_scaler.transform(X_test)\n",
    "X_train_scaled[0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train.values)\n",
    "y_val_enc   = label_encoder.transform(y_val.values)\n",
    "y_test_enc  = label_encoder.transform(y_test.values)\n",
    "y_train[:5].values, y_train_enc[:5], label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7099c0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6d34f",
   "metadata": {},
   "source": [
    "## Модель линейной классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    def __init__(self):\n",
    "        self.value = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, new_value):\n",
    "        self.count += 1\n",
    "        self.value = (1 / self.count) * new_value + (1 - 1 / self.count) * self.value\n",
    "        return self.value\n",
    "    \n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, l=0.1):\n",
    "        self.value = None\n",
    "        self.l = l\n",
    "\n",
    "    def update(self, new_value):\n",
    "        if self.value is not None:\n",
    "            self.value = self.l * new_value + (1 - self.l) * self.value\n",
    "        else:\n",
    "            self.value = new_value\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegNumpy():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_weights: list[list[float]] = None, # (n_features, n_classes)\n",
    "        initial_bias:    list[float] = None,       # (1, n_classes)\n",
    "        tolerance:       float = 1e-4,\n",
    "        early_stop: bool = False,\n",
    "        n_startup_rounds: int = 50,\n",
    "        early_stop_rounds: int = 50,\n",
    "        random_seed: int = SEED\n",
    "    ):\n",
    "        self.weights = (np.array(initial_weights) if initial_weights is not None \n",
    "                        else np.array([]))\n",
    "        self.bias = (np.array(initial_bias) if initial_bias is not None \n",
    "                     else np.array([]))\n",
    "        self.tolerance = tolerance\n",
    "        self.early_stop = early_stop\n",
    "        self.n_startup_rounds = n_startup_rounds\n",
    "        self.early_stop_rounds = early_stop_rounds\n",
    "        self.random_seed = random_seed\n",
    "        self.eps = 1e-9\n",
    "\n",
    "        self.rng_ = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "        # для рекуррентной оценки\n",
    "        self.rec_value = None\n",
    "        self.rec_count = 0\n",
    "        self.rec_history = []\n",
    "\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        features:       list[list[float]],\n",
    "        labels:         list[int],\n",
    "        learning_rate:  float = 1e-3,\n",
    "        total_steps:    int = 1000,\n",
    "        batch_size:     int | None = None, # None = full dataset\n",
    "        gd_algo:        Literal['gd', 'sgd'] = 'sgd',\n",
    "        momentum:       float = 0.0,\n",
    "        l2:             float = 0.0,\n",
    "        optim_step:     bool = False,\n",
    "        init_strategy:  Literal['normal', 'corr', 'multistart'] = 'normal',\n",
    "        return_weights_history:  bool = False,\n",
    "        verbose:                 bool = False,\n",
    "        # рекуррентная оценка функции потерь\n",
    "        rec_mode:                Literal['off','mean','ema'] = 'off',\n",
    "        ema_lambda:              float = 0.1,\n",
    "        # стратегия сэмплирования\n",
    "        sampling_mode:           Literal['uniform','by_margin'] = 'uniform',\n",
    "        shuffle:                 bool = True,\n",
    "        sampling_tau:            float = 0.2,\n",
    "        sampling_min_prob:       float = 0.01,\n",
    "        refresh_rate:            int = 100,\n",
    "        steps_per_epoch:         int | None = 100, # для логики логирования\n",
    "    ) -> None | list[list[float]]:\n",
    "\n",
    "        X = (np.array(features, dtype=np.float32).squeeze()\n",
    "            if not isinstance(features, np.ndarray)\n",
    "            else deepcopy(features).astype(np.float32, copy=False))\n",
    "        y = (np.array(labels, dtype=np.int64).squeeze()\n",
    "            if not isinstance(labels,  np.ndarray)\n",
    "            else deepcopy(labels).astype(np.int64,  copy=False))\n",
    "        if X.ndim == 1:\n",
    "            X = X[np.newaxis, :]\n",
    "        N = X.shape[0]\n",
    "\n",
    "        if gd_algo == 'gd':\n",
    "            batch_size = None\n",
    "\n",
    "        if batch_size is None:\n",
    "            batch_size = N\n",
    "\n",
    "        if steps_per_epoch is None:\n",
    "            steps_per_epoch = (N + batch_size - 1) // batch_size\n",
    "\n",
    "        # --- init\n",
    "        self._init_weights(X, y, init_strategy)\n",
    "        if return_weights_history:\n",
    "            weights_values = [self.weights.copy()]\n",
    "        Vdw = np.zeros_like(self.weights)\n",
    "        Vdb = np.zeros_like(self.bias)\n",
    "        self._rec_reset()\n",
    "        self.loss_values = []\n",
    "        no_improvement_counter = 0\n",
    "        rng = self.rng_\n",
    "            \n",
    "        def uniform_next_batch_stateful():\n",
    "            perm = rng.permutation(N) if shuffle else np.arange(N, dtype=np.int64)\n",
    "            ptr = 0\n",
    "\n",
    "            def get_batch():\n",
    "                nonlocal perm, ptr\n",
    "                if ptr >= N:\n",
    "                    perm = rng.permutation(N) if shuffle else np.arange(N, dtype=np.int64)\n",
    "                    ptr = 0\n",
    "\n",
    "                remaining = N - ptr\n",
    "                take = batch_size if batch_size <= remaining else remaining\n",
    "\n",
    "                idx = perm[ptr:ptr + take]\n",
    "                ptr += take\n",
    "                return idx\n",
    "\n",
    "            return get_batch\n",
    "\n",
    "        margin_probs = lambda: self._margin_sampling_probs(\n",
    "            X, y, use_abs=True, tau=sampling_tau, min_prob=sampling_min_prob\n",
    "        )\n",
    "\n",
    "        # ---- training loop (one unified path)\n",
    "        step = 0\n",
    "        block_loss_sum = 0.0\n",
    "        block_count = 0\n",
    "\n",
    "        if sampling_mode == 'uniform':\n",
    "            next_uniform_batch = uniform_next_batch_stateful()\n",
    "        elif sampling_mode == 'by_margin':\n",
    "            probs = margin_probs()\n",
    "            next_uniform_batch = None\n",
    "        else:\n",
    "            raise ValueError(\"sampling_mode must be 'uniform' or 'by_margin'\")\n",
    "\n",
    "        while step < total_steps:\n",
    "            if sampling_mode == 'uniform':\n",
    "                batch_idx = next_uniform_batch()\n",
    "            else:  # by_margin\n",
    "                if step % refresh_rate == 0:\n",
    "                    probs = margin_probs()\n",
    "                batch_idx = rng.choice(N, size=batch_size, replace=True, p=probs)\n",
    "\n",
    "            xi = X[batch_idx, :]\n",
    "            yi = y[batch_idx]\n",
    "\n",
    "            # forward / loss\n",
    "            logits = self.forward(xi)\n",
    "            loss   = self._loss_fn_opt(yi, logits, reduction=None)\n",
    "\n",
    "            # Self-Normalized Importance Sampling Loss\n",
    "            if sampling_mode == 'by_margin':\n",
    "                pi = probs[batch_idx]\n",
    "                sample_weights = 1.0 / np.clip(pi, 1e-12, None)\n",
    "                loss = (loss * sample_weights).sum() / sample_weights.sum()\n",
    "            else:\n",
    "                sample_weights = None\n",
    "                loss = loss.mean()\n",
    "                \n",
    "            block_loss_sum += loss\n",
    "            block_count    += 1\n",
    "\n",
    "            # recurrent quality update\n",
    "            rec_val = self._rec_update(loss, mode=rec_mode, ema_lambda=ema_lambda)\n",
    "            self.rec_history.append(rec_val)\n",
    "\n",
    "            # gradients\n",
    "            w_grad, b_grad = self._gradient(xi, yi, logits)\n",
    "            # L2 - regularization\n",
    "            if l2 > 0.0:\n",
    "                w_grad += l2 * self.weights\n",
    "\n",
    "            # momentum (EMA style)\n",
    "            Vdw = momentum * Vdw - (1.0 - momentum) * w_grad\n",
    "            Vdb = momentum * Vdb - (1.0 - momentum) * b_grad\n",
    "            \n",
    "            if optim_step:\n",
    "                learning_rate = self._line_search_backtracking(\n",
    "                    xi, yi, w_grad, b_grad, Vdw, Vdb, l2=l2\n",
    "                )\n",
    "\n",
    "            # update\n",
    "            self.weights += learning_rate * Vdw\n",
    "            self.bias    += learning_rate * Vdb\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            # logging once per “epoch-sized” number of steps\n",
    "            if block_count >= steps_per_epoch:\n",
    "                mean_block_loss = block_loss_sum / block_count\n",
    "                self.loss_values.append(mean_block_loss)\n",
    "\n",
    "                # # recurrent quality update\n",
    "                # rec_val = self._rec_update(mean_block_loss, mode=rec_mode, ema_lambda=ema_lambda)\n",
    "                # self.rec_history.append(rec_val)\n",
    "\n",
    "                if return_weights_history:\n",
    "                    weights_values.append(self.weights.copy())\n",
    "                if verbose:\n",
    "                    print(f\"step {step:6d} | block_loss={mean_block_loss:.6f} | batch_size={batch_size} | mode={sampling_mode}\")\n",
    "\n",
    "                block_loss_sum = 0.0\n",
    "                block_count = 0\n",
    "\n",
    "            # early stopping on monitored series (smoothed if rec_mode != 'off')\n",
    "            if self.early_stop and step > self.n_startup_rounds + 1:\n",
    "                if 0 < (self.rec_history[-2] - self.rec_history[-1]) < self.tolerance:\n",
    "                    no_improvement_counter += 1\n",
    "                    if no_improvement_counter >= self.early_stop_rounds:\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping at step {step}\")\n",
    "                        break\n",
    "                else:\n",
    "                    no_improvement_counter = 0\n",
    "\n",
    "        if return_weights_history:\n",
    "            return np.array(weights_values)\n",
    "        \n",
    "        \n",
    "    def _line_search_backtracking(\n",
    "        self,\n",
    "        X, y, \n",
    "        grad_w: np.ndarray, grad_b: np.ndarray, \n",
    "        dir_w:  np.ndarray, dir_b:  np.ndarray,\n",
    "        step: float = 1.0, \n",
    "        alpha: float = 1e-4, \n",
    "        beta: float = 0.5, \n",
    "        tol: float = 1e-8,\n",
    "        l2: float = 0.0,\n",
    "        default_lr: float = 1e-9,\n",
    "        eps: float = 1e-12\n",
    "    ):\n",
    "        logits_0 = self.forward(X)\n",
    "        loss_0 = self._loss_fn_opt(y, logits_0, reduction='mean')\n",
    "        if l2 > 0.0:\n",
    "            loss_0 += l2 * np.sum(np.pow(self.weights, 2)) * 0.5\n",
    "        \n",
    "        W0 = self.weights\n",
    "        b0 = self.bias\n",
    "\n",
    "        # directional derivatives\n",
    "        dw = (grad_w * dir_w).sum()\n",
    "        db = (grad_b * dir_b).sum()\n",
    "        dd = dw + db\n",
    "        \n",
    "        if l2 > 0.0:\n",
    "            dd += l2 * (W0 * dir_w).sum()\n",
    "\n",
    "        if dd >= eps:\n",
    "            if np.allclose(dir_w, -grad_w) and np.allclose(dir_b, -grad_b):\n",
    "                return 0.0\n",
    "            # switching to regular gradient descent\n",
    "            np.copyto(dst=dir_w, src=-grad_w); np.copyto(dst=dir_b, src=-grad_b)\n",
    "            dd = (grad_w * dir_w).sum() + (grad_b * dir_b).sum()\n",
    "            if l2 > 0.0:\n",
    "                dd += l2 * (W0 * dir_w).sum()\n",
    "            if dd >= eps:\n",
    "                return 0.0\n",
    "\n",
    "        t = step\n",
    "\n",
    "        # TODO: добавить оптимизации через раскрытие logits_t и l2_norm и предварительного подсчета неизменных членов\n",
    "        while t > tol:\n",
    "            Wt = W0 + t * dir_w\n",
    "            bt = b0 + t * dir_b\n",
    "            logits_t = np.matmul(X, Wt) + bt\n",
    "            loss_t = self._loss_fn_opt(y, logits_t, reduction='mean')\n",
    "            if l2 > 0.0:\n",
    "                loss_t += l2 * np.sum(np.pow(Wt, 2)) * 0.5\n",
    "\n",
    "            if loss_t <= loss_0 + alpha * t * dd:\n",
    "                return t\n",
    "            \n",
    "            t *= beta\n",
    "\n",
    "        return default_lr\n",
    "        \n",
    "\n",
    "    def predict(self, features: list[list[float]]):\n",
    "        X = (np.array(features).squeeze() if not isinstance(features, np.ndarray) \n",
    "             else deepcopy(features).astype(np.float32, copy=False))\n",
    "        if X.ndim == 1:\n",
    "            X = X[np.newaxis, :]\n",
    "        logits = self.forward(X) # (n_samples, n_classes)\n",
    "        probs  = self._softmax(logits) # не обязательно\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def _create_onehot_target(self, y: np.array):\n",
    "        ohe_enc = OneHotEncoder(categories=[np.unique(y)], sparse_output=False)\n",
    "        y_enc = ohe_enc.fit_transform(y.reshape(-1, 1))\n",
    "        return y_enc # output -> (n_samples, n_classes)\n",
    "    \n",
    "    def _init_weights(\n",
    "        self, X: np.ndarray, y: np.ndarray,\n",
    "        init_strategy: Literal['normal', 'corr', 'multistart'] = 'normal',\n",
    "        n_starts: int = 5, search_steps: int = 50, lr: float = 1e-2,\n",
    "    ):\n",
    "        N, d = X.shape\n",
    "        K = np.max(y) + 1\n",
    "\n",
    "        if init_strategy == 'normal':\n",
    "            if self.weights.size == 0:\n",
    "                self.weights = self.rng_.standard_normal((d, K), dtype=np.float32)\n",
    "            if self.bias.size == 0:\n",
    "                self.bias = self.rng_.standard_normal((1, K), dtype=np.float32)\n",
    "            return\n",
    "\n",
    "        if init_strategy == 'corr':\n",
    "            if self.weights.size != 0 and self.bias.size != 0:\n",
    "                return\n",
    "            # евклидова норма\n",
    "            # denom = np.sum(X * X, axis=0)       # shape (d,)\n",
    "            denom = np.float64(N)\n",
    "\n",
    "            W = np.zeros((d, K), dtype=np.float64)\n",
    "            b = np.zeros((1, K), dtype=np.float64)\n",
    "\n",
    "            for k in range(K):\n",
    "                t = (y == k).astype(np.float64) # 1 for class k, else 0\n",
    "                # weights: elementwise division by per-feature squared norm\n",
    "                # With centered X, X^T t == X^T (t - mean(t)), so no need to center t explicitly.\n",
    "                numer = X.T @ t                           # shape (d,)\n",
    "                W[:, k] = numer / denom\n",
    "\n",
    "                # intercept: with centered features, LS gives b_k = mean(t^{(k)})\n",
    "                b[0, k] = t.mean()\n",
    "\n",
    "            if self.weights.size == 0:\n",
    "                self.weights = W.astype(np.float32, copy=False)\n",
    "            if self.bias.size == 0:\n",
    "                self.bias = b.astype(np.float32, copy=False)\n",
    "            return\n",
    "        \n",
    "        if init_strategy == 'multistart':\n",
    "            best_loss = np.inf\n",
    "            best_W, best_b = None, None\n",
    "\n",
    "            for _ in range(n_starts):\n",
    "                W = self.rng_.standard_normal((d, K), dtype=np.float32)\n",
    "                b = self.rng_.standard_normal((1, K), dtype=np.float32)\n",
    "                \n",
    "                # short warmup\n",
    "                w, b, loss = self._warmup(X, y, W, b, steps=search_steps, lr=lr)\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_W, best_b = w, b\n",
    "\n",
    "                self.weights = best_W\n",
    "                self.bias    = best_b\n",
    "            return\n",
    "        \n",
    "        raise ValueError(\"init_strategy must be 'normal' or 'corr'\")\n",
    "    \n",
    "    def _warmup(self, X, y, W, b, steps=50, lr=1e-2):\n",
    "        W = W.copy(); b = b.copy()\n",
    "        for _ in range(steps):\n",
    "            logits = np.matmul(X, W) + b\n",
    "            loss   = self._loss_fn_opt(y, logits, reduction='mean')\n",
    "            w_grad, b_grad = self._gradient(X, y, logits)\n",
    "            W -= lr * w_grad\n",
    "            b -= lr * b_grad\n",
    "        return W, b, float(loss)\n",
    "    \n",
    "        \n",
    "    def _softmax(self, X: np.array) -> np.array:\n",
    "        Z = X - np.max(X, axis=1, keepdims=True)\n",
    "        numerator = np.exp(Z)\n",
    "        denominator = np.sum(numerator, axis=1, keepdims=True)\n",
    "        softmax_probs = numerator / denominator\n",
    "        return softmax_probs # -> (n_samples, n_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # (n_samples, n_features) * (n_features, n_classes)\n",
    "        logits = np.matmul(X, self.weights) + self.bias # -> (n_samples, n_classes)\n",
    "        return logits\n",
    "    \n",
    "    # def loss_fn_expanded(self, X, y_true):\n",
    "    #     # (n_samples, n_features) * (n_features, n_classes) + (n_samples, 1) * (1, n_classes) = (n_samples, n_classes)\n",
    "    #     logits = np.matmul(X, self.weights) + np.matmul(np.ones((X.shape[0], 1)), self.bias)\n",
    "    #     exp_logits = np.exp(logits)\n",
    "    #     logits_sum = np.sum(exp_logits, axis=1) # -> (n_samples, 1)\n",
    "    #     # (n_samples, n_classes) * (n_samples, n_classes)\n",
    "    #     true_class_logits = logits[np.arange(X.shape[0]), y_true]\n",
    "    #     return np.mean(np.log(logits_sum) - true_class_logits)\n",
    "\n",
    "    # def loss_fn(self, y_true, logits):\n",
    "    #     log_probs = np.log(self.softmax(logits)) # -> (n_samples, classes)\n",
    "    #     # y_true_ohe = self.create_onehot_target(y_true) # -> (n_samples, classes)\n",
    "    #     # likelihood = (log_probs * y_true_ohe).sum(axis=1).mean()\n",
    "    #     likelihood = (log_probs[np.arange(log_probs.shape[0]), y_true]).mean()\n",
    "    #     return -likelihood\n",
    "    \n",
    "    def _loss_fn_opt(self, y_true, logits, reduction=None):\n",
    "        lse = logsumexp(logits, axis=1, keepdims=True)\n",
    "        nll = lse - logits\n",
    "        loss = nll[np.arange(nll.shape[0]), y_true]\n",
    "        if reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "    \n",
    "    def _rec_reset(self):\n",
    "        self.rec_value = None\n",
    "        self.rec_count = 0\n",
    "        self.rec_history = []\n",
    "\n",
    "    def _rec_update(self, xi, mode=\"off\", ema_lambda=0.1):\n",
    "        if mode == \"off\":\n",
    "            return xi\n",
    "\n",
    "        if self.rec_value is None:\n",
    "            # инициализация последовательности\n",
    "            self.rec_value = xi\n",
    "            self.rec_count = 1\n",
    "            return self.rec_value\n",
    "\n",
    "        if mode == \"mean\":\n",
    "            # running mean: Q_m = (1/m)*xi_m + (1 - 1/m)*Q_{m-1}\n",
    "            self.rec_count += 1\n",
    "            m = self.rec_count\n",
    "            self.rec_value = (1.0/m)*xi + (1.0 - 1.0/m)*self.rec_value\n",
    "            return self.rec_value\n",
    "\n",
    "        if mode == \"ema\":\n",
    "            # EMA: Q_m = λ xi_m + (1 - λ) Q_{m-1}\n",
    "            self.rec_value = ema_lambda * xi + (1.0 - ema_lambda) * self.rec_value\n",
    "            return self.rec_value\n",
    "\n",
    "        return xi\n",
    "\n",
    "    def _gradient(self, X, y_true, logits):\n",
    "        y_prob = self._softmax(logits)\n",
    "        y_prob[np.arange(y_prob.shape[0]), y_true] -= 1\n",
    "        y_prob /= y_prob.shape[0]\n",
    "        w_grad = np.matmul(X.T, y_prob)\n",
    "        b_grad = y_prob.sum(axis=0, keepdims=True)\n",
    "        return w_grad, b_grad\n",
    "    \n",
    "\n",
    "    def calc_margins(self, X, y_true, plot: bool = False, eps=1e-3, **kwargs):\n",
    "        logits = self.forward(X)\n",
    "        true_logits = logits[np.arange(X.shape[0]), y_true]\n",
    "        logits[np.arange(logits.shape[0]), y_true] = -np.inf\n",
    "        false_logits = logits.max(axis=1)\n",
    "        margins = true_logits - false_logits\n",
    "\n",
    "        if plot:\n",
    "            \n",
    "            sorted_idx = np.argsort(margins)\n",
    "            sorted_margins = margins[sorted_idx]\n",
    "            \n",
    "            line_kwargs      = {'lw': 2}\n",
    "            pos_fill_kwargs  = {'alpha': 0.25, 'color': 'tab:green'}\n",
    "            neg_fill_kwargs  = {'alpha': 0.25, 'color': 'tab:red'}\n",
    "            zero_fill_kwargs = {'alpha': 0.25, 'color': 'gold'}\n",
    "\n",
    "            # masks\n",
    "            if eps > 0.0:\n",
    "                mask_zero = np.abs(sorted_margins) <= eps\n",
    "                mask_pos  = sorted_margins >  eps\n",
    "                mask_neg  = sorted_margins < -eps\n",
    "            else:\n",
    "                mask_zero = np.zeros_like(sorted_margins, dtype=bool)\n",
    "                mask_pos  = sorted_margins > 0\n",
    "                mask_neg  = sorted_margins < 0\n",
    "\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            # line\n",
    "            plot_idx = np.arange(sorted_margins.shape[0])\n",
    "            plt.plot(plot_idx, sorted_margins, **line_kwargs)\n",
    "            plt.axhline(0.0, color='black', lw=1, alpha=0.7)\n",
    "\n",
    "            if np.any(mask_neg):\n",
    "                plt.fill_between(plot_idx, sorted_margins, 0.0, where=mask_neg, interpolate=True, **neg_fill_kwargs)\n",
    "            if np.any(mask_zero):\n",
    "                plt.fill_between(plot_idx, sorted_margins, 0.0, where=mask_zero, interpolate=True, **zero_fill_kwargs)\n",
    "            if np.any(mask_pos):\n",
    "                plt.fill_between(plot_idx, sorted_margins, 0.0, where=mask_pos, interpolate=True, **pos_fill_kwargs)\n",
    "\n",
    "            plt.xlabel(\"sample index (sorted)\")\n",
    "            plt.ylabel(\"margin\")\n",
    "            plt.title(\"Margin curve with signed areas\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        return margins\n",
    "    \n",
    "    # TODO: сделать сэмплер с разной логикой выбора сложных случаев\n",
    "    # 1) -abs(margins) - для любых (правильных или нет) случаев с малой долей уверенности\n",
    "    # 2) -margins вместо -abs(margins) - для точно неправильно классифицированных случаев\n",
    "    def _margin_sampling_probs(\n",
    "        self, X, y, use_abs: bool = True, tau: float = 0.2, min_prob: float = 0.01\n",
    "    ):\n",
    "        margins = self.calc_margins(X, y)\n",
    "\n",
    "        diff = -np.abs(margins) if use_abs else -margins\n",
    "        scores = diff / max(tau, 1e-8)\n",
    "        probs = self._softmax(scores.reshape(1, -1)).squeeze()\n",
    "\n",
    "        floor = min_prob / X.shape[0]\n",
    "        probs = (1.0 - min_prob) * probs + floor\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogRegNumpy(early_stop=True)\n",
    "# logreg._init_weights(X_train_scaled, y_train_enc, 'multistart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(\n",
    "    X_train_scaled, y_train_enc,\n",
    "    learning_rate=0.01, total_steps=X_train_scaled.shape[0]*3, init_strategy='corr',\n",
    "    batch_size=X_train_scaled.shape[0], momentum=0.99, l2=0.0, optim_step=True, verbose=True, rec_mode='ema', ema_lambda=0.01,\n",
    "    sampling_mode='uniform', shuffle=True, sampling_tau=0.2, sampling_min_prob=0.01, refresh_rate=200,\n",
    "    steps_per_epoch=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfaad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logreg.loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logreg.rec_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = logreg.calc_margins(X_train_scaled, y_train_enc, plot=True, eps=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7471460",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = logreg.predict(X_train_scaled)\n",
    "preds_test  = logreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93eb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train accuracy: {(preds_train == y_train_enc).mean()*100.:.4f}')\n",
    "print(f'Test accuracy: {(preds_test == y_test_enc).mean()*100.:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15fc241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df0e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl python 3.11.5",
   "language": "python",
   "name": "wsl_main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
