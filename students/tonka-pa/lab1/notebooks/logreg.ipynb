{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d24d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "from time import perf_counter\n",
    "from copy import deepcopy\n",
    "from typing import Literal\n",
    "from itertools import cycle\n",
    "from tabulate import tabulate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support, classification_report,\n",
    "                             roc_curve, auc, roc_auc_score, confusion_matrix, RocCurveDisplay)\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ccdebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9\n",
    "\n",
    "SEED = 18092025\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1348d94a",
   "metadata": {},
   "source": [
    "[Страница датасета](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf18d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing file: ../datasets/data.csv\n"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/697/predict+students+dropout+and+academic+success.zip\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "saveto_path = Path('../datasets')\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    for file in z.namelist():\n",
    "        target_path = saveto_path / file\n",
    "        if target_path.exists():\n",
    "            print(f\"Skipping existing file: {target_path}\")\n",
    "        else:\n",
    "            z.extract(file, saveto_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e0b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = Path(\"../source/utils/model_params.json\")\n",
    "\n",
    "with json_path.open(mode='r', encoding='utf-8') as file:\n",
    "    params_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146bf6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = Path(\"../source/utils/model_params.yaml\")\n",
    "\n",
    "with yaml_path.open(mode='r', encoding='utf-8') as file:\n",
    "    params_yaml = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f2497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('../datasets/data.csv')\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path, delimiter=';')\n",
    "except Exception:\n",
    "    print('No such file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d82fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.max_colwidth')\n",
    "# pd.reset_option('display.width')\n",
    "# pd.reset_option('display.expand_frame_repr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628b879",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb670c",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46bbea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nans(df):\n",
    "    nans_per_col = [(col, df[col].isna().sum(), df[col].isna().sum() / df.shape[0] * 100) for col in df.columns]\n",
    "    dtype = [('col_name', 'U20'), ('nans', int), ('nans_perc', float)]\n",
    "    nans_per_col = np.array(nans_per_col, dtype=dtype)\n",
    "    nans_per_col = nans_per_col[nans_per_col['nans'] > 0]\n",
    "    nans_per_col = np.sort(nans_per_col, order='nans')\n",
    "\n",
    "    if nans_per_col.shape[0] == 0:\n",
    "        print('No nans in the dataset')\n",
    "        return\n",
    "\n",
    "    df_show = pd.DataFrame(nans_per_col[::-1])\n",
    "    display(df_show.style.background_gradient(cmap='Blues'))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "    y_pos = np.arange(len(nans_per_col))\n",
    "    \n",
    "    ax.barh(y_pos, nans_per_col['nans_perc'], alpha=0.8, edgecolor='black', linewidth=1) \n",
    "    ax.set_yticks(y_pos, labels=nans_per_col['col_name'])\n",
    "    ax.set_xlabel('Nans, %', fontsize=14)\n",
    "    ax.set_title('Nans rate for each column', fontsize=16)\n",
    "    ax.set_xlim(0, min(np.max(df_show['nans_perc']) + 5.0, 100.0))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "    ax.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0d3aa7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marital status</th>\n",
       "      <th>Application mode</th>\n",
       "      <th>Application order</th>\n",
       "      <th>Course</th>\n",
       "      <th>Daytime/evening attendance\\t</th>\n",
       "      <th>Previous qualification</th>\n",
       "      <th>Previous qualification (grade)</th>\n",
       "      <th>Nacionality</th>\n",
       "      <th>Mother's qualification</th>\n",
       "      <th>Father's qualification</th>\n",
       "      <th>Mother's occupation</th>\n",
       "      <th>Father's occupation</th>\n",
       "      <th>Admission grade</th>\n",
       "      <th>Displaced</th>\n",
       "      <th>Educational special needs</th>\n",
       "      <th>Debtor</th>\n",
       "      <th>Tuition fees up to date</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Scholarship holder</th>\n",
       "      <th>Age at enrollment</th>\n",
       "      <th>International</th>\n",
       "      <th>Curricular units 1st sem (credited)</th>\n",
       "      <th>Curricular units 1st sem (enrolled)</th>\n",
       "      <th>Curricular units 1st sem (evaluations)</th>\n",
       "      <th>Curricular units 1st sem (approved)</th>\n",
       "      <th>Curricular units 1st sem (grade)</th>\n",
       "      <th>Curricular units 1st sem (without evaluations)</th>\n",
       "      <th>Curricular units 2nd sem (credited)</th>\n",
       "      <th>Curricular units 2nd sem (enrolled)</th>\n",
       "      <th>Curricular units 2nd sem (evaluations)</th>\n",
       "      <th>Curricular units 2nd sem (approved)</th>\n",
       "      <th>Curricular units 2nd sem (grade)</th>\n",
       "      <th>Curricular units 2nd sem (without evaluations)</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>Inflation rate</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>127.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>142.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9070</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>124.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>9773</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>119.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>13.428571</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>8014</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>141.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Marital status  Application mode  Application order  Course  Daytime/evening attendance\\t  Previous qualification  Previous qualification (grade)  Nacionality  Mother's qualification  Father's qualification  Mother's occupation  Father's occupation  Admission grade  Displaced  Educational special needs  Debtor  Tuition fees up to date  Gender  Scholarship holder  Age at enrollment  International  Curricular units 1st sem (credited)  Curricular units 1st sem (enrolled)  Curricular units 1st sem (evaluations)  Curricular units 1st sem (approved)  Curricular units 1st sem (grade)  Curricular units 1st sem (without evaluations)  Curricular units 2nd sem (credited)  Curricular units 2nd sem (enrolled)  Curricular units 2nd sem (evaluations)  Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  Curricular units 2nd sem (without evaluations)  Unemployment rate  Inflation rate   GDP    Target\n",
       "0               1                17                  5     171                             1                       1                           122.0            1                      19                      12                    5                    9            127.3          1                          0       0                        1       1                   0                 20              0                                    0                                    0                                       0                                    0                          0.000000                                               0                                    0                                    0                                       0                                    0                          0.000000                                               0               10.8             1.4  1.74   Dropout\n",
       "1               1                15                  1    9254                             1                       1                           160.0            1                       1                       3                    3                    3            142.5          1                          0       0                        0       1                   0                 19              0                                    0                                    6                                       6                                    6                         14.000000                                               0                                    0                                    6                                       6                                    6                         13.666667                                               0               13.9            -0.3  0.79  Graduate\n",
       "2               1                 1                  5    9070                             1                       1                           122.0            1                      37                      37                    9                    9            124.8          1                          0       0                        0       1                   0                 19              0                                    0                                    6                                       0                                    0                          0.000000                                               0                                    0                                    6                                       0                                    0                          0.000000                                               0               10.8             1.4  1.74   Dropout\n",
       "3               1                17                  2    9773                             1                       1                           122.0            1                      38                      37                    5                    3            119.6          1                          0       0                        1       0                   0                 20              0                                    0                                    6                                       8                                    6                         13.428571                                               0                                    0                                    6                                      10                                    5                         12.400000                                               0                9.4            -0.8 -3.12  Graduate\n",
       "4               2                39                  1    8014                             0                       1                           100.0            1                      37                      38                    9                    9            141.5          0                          0       0                        1       0                   0                 45              0                                    0                                    6                                       9                                    5                         12.333333                                               0                                    0                                    6                                       6                                    6                         13.000000                                               0               13.9            -0.3  0.79  Graduate"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (4424, 37)\n"
     ]
    }
   ],
   "source": [
    "display(df.head())\n",
    "print('Dataset shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef577992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_names_transform(col_name: str) -> str:\n",
    "    res_name = col_name.strip().replace(\"\\t\", \"\").replace(' ', '_').lower()\n",
    "    return res_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "775e9b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['marital_status', 'application_mode', 'application_order', 'course',\n",
       "       'daytime/evening_attendance', 'previous_qualification',\n",
       "       'previous_qualification_(grade)', 'nacionality',\n",
       "       'mother's_qualification', 'father's_qualification',\n",
       "       'mother's_occupation', 'father's_occupation', 'admission_grade',\n",
       "       'displaced', 'educational_special_needs', 'debtor',\n",
       "       'tuition_fees_up_to_date', 'gender', 'scholarship_holder',\n",
       "       'age_at_enrollment', 'international',\n",
       "       'curricular_units_1st_sem_(credited)',\n",
       "       'curricular_units_1st_sem_(enrolled)',\n",
       "       'curricular_units_1st_sem_(evaluations)',\n",
       "       'curricular_units_1st_sem_(approved)',\n",
       "       'curricular_units_1st_sem_(grade)',\n",
       "       'curricular_units_1st_sem_(without_evaluations)',\n",
       "       'curricular_units_2nd_sem_(credited)',\n",
       "       'curricular_units_2nd_sem_(enrolled)',\n",
       "       'curricular_units_2nd_sem_(evaluations)',\n",
       "       'curricular_units_2nd_sem_(approved)',\n",
       "       'curricular_units_2nd_sem_(grade)',\n",
       "       'curricular_units_2nd_sem_(without_evaluations)', 'unemployment_rate',\n",
       "       'inflation_rate', 'gdp', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = map(col_names_transform, df.columns.values)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a07cb1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marital_status</th>\n",
       "      <th>application_mode</th>\n",
       "      <th>application_order</th>\n",
       "      <th>course</th>\n",
       "      <th>daytime/evening_attendance</th>\n",
       "      <th>previous_qualification</th>\n",
       "      <th>previous_qualification_(grade)</th>\n",
       "      <th>nacionality</th>\n",
       "      <th>mother's_qualification</th>\n",
       "      <th>father's_qualification</th>\n",
       "      <th>mother's_occupation</th>\n",
       "      <th>father's_occupation</th>\n",
       "      <th>admission_grade</th>\n",
       "      <th>displaced</th>\n",
       "      <th>educational_special_needs</th>\n",
       "      <th>debtor</th>\n",
       "      <th>tuition_fees_up_to_date</th>\n",
       "      <th>gender</th>\n",
       "      <th>scholarship_holder</th>\n",
       "      <th>age_at_enrollment</th>\n",
       "      <th>international</th>\n",
       "      <th>curricular_units_1st_sem_(credited)</th>\n",
       "      <th>curricular_units_1st_sem_(enrolled)</th>\n",
       "      <th>curricular_units_1st_sem_(evaluations)</th>\n",
       "      <th>curricular_units_1st_sem_(approved)</th>\n",
       "      <th>curricular_units_1st_sem_(grade)</th>\n",
       "      <th>curricular_units_1st_sem_(without_evaluations)</th>\n",
       "      <th>curricular_units_2nd_sem_(credited)</th>\n",
       "      <th>curricular_units_2nd_sem_(enrolled)</th>\n",
       "      <th>curricular_units_2nd_sem_(evaluations)</th>\n",
       "      <th>curricular_units_2nd_sem_(approved)</th>\n",
       "      <th>curricular_units_2nd_sem_(grade)</th>\n",
       "      <th>curricular_units_2nd_sem_(without_evaluations)</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>inflation_rate</th>\n",
       "      <th>gdp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "      <td>4424.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.178571</td>\n",
       "      <td>18.669078</td>\n",
       "      <td>1.727848</td>\n",
       "      <td>8856.642631</td>\n",
       "      <td>0.890823</td>\n",
       "      <td>4.577758</td>\n",
       "      <td>132.613314</td>\n",
       "      <td>1.873192</td>\n",
       "      <td>19.561935</td>\n",
       "      <td>22.275316</td>\n",
       "      <td>10.960895</td>\n",
       "      <td>11.032324</td>\n",
       "      <td>126.978119</td>\n",
       "      <td>0.548373</td>\n",
       "      <td>0.011528</td>\n",
       "      <td>0.113698</td>\n",
       "      <td>0.880651</td>\n",
       "      <td>0.351718</td>\n",
       "      <td>0.248418</td>\n",
       "      <td>23.265145</td>\n",
       "      <td>0.024864</td>\n",
       "      <td>0.709991</td>\n",
       "      <td>6.270570</td>\n",
       "      <td>8.299051</td>\n",
       "      <td>4.706600</td>\n",
       "      <td>10.640822</td>\n",
       "      <td>0.137658</td>\n",
       "      <td>0.541817</td>\n",
       "      <td>6.232143</td>\n",
       "      <td>8.063291</td>\n",
       "      <td>4.435805</td>\n",
       "      <td>10.230206</td>\n",
       "      <td>0.150316</td>\n",
       "      <td>11.566139</td>\n",
       "      <td>1.228029</td>\n",
       "      <td>0.001969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.605747</td>\n",
       "      <td>17.484682</td>\n",
       "      <td>1.313793</td>\n",
       "      <td>2063.566416</td>\n",
       "      <td>0.311897</td>\n",
       "      <td>10.216592</td>\n",
       "      <td>13.188332</td>\n",
       "      <td>6.914514</td>\n",
       "      <td>15.603186</td>\n",
       "      <td>15.343108</td>\n",
       "      <td>26.418253</td>\n",
       "      <td>25.263040</td>\n",
       "      <td>14.482001</td>\n",
       "      <td>0.497711</td>\n",
       "      <td>0.106760</td>\n",
       "      <td>0.317480</td>\n",
       "      <td>0.324235</td>\n",
       "      <td>0.477560</td>\n",
       "      <td>0.432144</td>\n",
       "      <td>7.587816</td>\n",
       "      <td>0.155729</td>\n",
       "      <td>2.360507</td>\n",
       "      <td>2.480178</td>\n",
       "      <td>4.179106</td>\n",
       "      <td>3.094238</td>\n",
       "      <td>4.843663</td>\n",
       "      <td>0.690880</td>\n",
       "      <td>1.918546</td>\n",
       "      <td>2.195951</td>\n",
       "      <td>3.947951</td>\n",
       "      <td>3.014764</td>\n",
       "      <td>5.210808</td>\n",
       "      <td>0.753774</td>\n",
       "      <td>2.663850</td>\n",
       "      <td>1.382711</td>\n",
       "      <td>2.269935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-4.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9085.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>117.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>-1.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9238.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>133.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>126.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9556.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>134.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9991.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>18.875000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>3.510000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       marital_status  application_mode  application_order       course  daytime/evening_attendance  previous_qualification  previous_qualification_(grade)  nacionality  mother's_qualification  father's_qualification  mother's_occupation  father's_occupation  admission_grade    displaced  educational_special_needs       debtor  tuition_fees_up_to_date       gender  scholarship_holder  age_at_enrollment  international  curricular_units_1st_sem_(credited)  curricular_units_1st_sem_(enrolled)  curricular_units_1st_sem_(evaluations)  curricular_units_1st_sem_(approved)  curricular_units_1st_sem_(grade)  curricular_units_1st_sem_(without_evaluations)  curricular_units_2nd_sem_(credited)  curricular_units_2nd_sem_(enrolled)  curricular_units_2nd_sem_(evaluations)  curricular_units_2nd_sem_(approved)  curricular_units_2nd_sem_(grade)  curricular_units_2nd_sem_(without_evaluations)  unemployment_rate  inflation_rate          gdp\n",
       "count     4424.000000       4424.000000        4424.000000  4424.000000                 4424.000000             4424.000000                     4424.000000  4424.000000             4424.000000             4424.000000          4424.000000          4424.000000      4424.000000  4424.000000                4424.000000  4424.000000              4424.000000  4424.000000         4424.000000        4424.000000    4424.000000                          4424.000000                          4424.000000                             4424.000000                          4424.000000                       4424.000000                                     4424.000000                          4424.000000                          4424.000000                             4424.000000                          4424.000000                       4424.000000                                     4424.000000        4424.000000     4424.000000  4424.000000\n",
       "mean         1.178571         18.669078           1.727848  8856.642631                    0.890823                4.577758                      132.613314     1.873192               19.561935               22.275316            10.960895            11.032324       126.978119     0.548373                   0.011528     0.113698                 0.880651     0.351718            0.248418          23.265145       0.024864                             0.709991                             6.270570                                8.299051                             4.706600                         10.640822                                        0.137658                             0.541817                             6.232143                                8.063291                             4.435805                         10.230206                                        0.150316          11.566139        1.228029     0.001969\n",
       "std          0.605747         17.484682           1.313793  2063.566416                    0.311897               10.216592                       13.188332     6.914514               15.603186               15.343108            26.418253            25.263040        14.482001     0.497711                   0.106760     0.317480                 0.324235     0.477560            0.432144           7.587816       0.155729                             2.360507                             2.480178                                4.179106                             3.094238                          4.843663                                        0.690880                             1.918546                             2.195951                                3.947951                             3.014764                          5.210808                                        0.753774           2.663850        1.382711     2.269935\n",
       "min          1.000000          1.000000           0.000000    33.000000                    0.000000                1.000000                       95.000000     1.000000                1.000000                1.000000             0.000000             0.000000        95.000000     0.000000                   0.000000     0.000000                 0.000000     0.000000            0.000000          17.000000       0.000000                             0.000000                             0.000000                                0.000000                             0.000000                          0.000000                                        0.000000                             0.000000                             0.000000                                0.000000                             0.000000                          0.000000                                        0.000000           7.600000       -0.800000    -4.060000\n",
       "25%          1.000000          1.000000           1.000000  9085.000000                    1.000000                1.000000                      125.000000     1.000000                2.000000                3.000000             4.000000             4.000000       117.900000     0.000000                   0.000000     0.000000                 1.000000     0.000000            0.000000          19.000000       0.000000                             0.000000                             5.000000                                6.000000                             3.000000                         11.000000                                        0.000000                             0.000000                             5.000000                                6.000000                             2.000000                         10.750000                                        0.000000           9.400000        0.300000    -1.700000\n",
       "50%          1.000000         17.000000           1.000000  9238.000000                    1.000000                1.000000                      133.100000     1.000000               19.000000               19.000000             5.000000             7.000000       126.100000     1.000000                   0.000000     0.000000                 1.000000     0.000000            0.000000          20.000000       0.000000                             0.000000                             6.000000                                8.000000                             5.000000                         12.285714                                        0.000000                             0.000000                             6.000000                                8.000000                             5.000000                         12.200000                                        0.000000          11.100000        1.400000     0.320000\n",
       "75%          1.000000         39.000000           2.000000  9556.000000                    1.000000                1.000000                      140.000000     1.000000               37.000000               37.000000             9.000000             9.000000       134.800000     1.000000                   0.000000     0.000000                 1.000000     1.000000            0.000000          25.000000       0.000000                             0.000000                             7.000000                               10.000000                             6.000000                         13.400000                                        0.000000                             0.000000                             7.000000                               10.000000                             6.000000                         13.333333                                        0.000000          13.900000        2.600000     1.790000\n",
       "max          6.000000         57.000000           9.000000  9991.000000                    1.000000               43.000000                      190.000000   109.000000               44.000000               44.000000           194.000000           195.000000       190.000000     1.000000                   1.000000     1.000000                 1.000000     1.000000            1.000000          70.000000       1.000000                            20.000000                            26.000000                               45.000000                            26.000000                         18.875000                                       12.000000                            19.000000                            23.000000                               33.000000                            20.000000                         18.571429                                       12.000000          16.200000        3.700000     3.510000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a095d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: добавить другие стат. показатели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36286643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nans in the dataset\n"
     ]
    }
   ],
   "source": [
    "display_nans(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbba5b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marital_status                                      int64\n",
       "application_mode                                    int64\n",
       "application_order                                   int64\n",
       "course                                              int64\n",
       "daytime/evening_attendance                          int64\n",
       "previous_qualification                              int64\n",
       "previous_qualification_(grade)                    float64\n",
       "nacionality                                         int64\n",
       "mother's_qualification                              int64\n",
       "father's_qualification                              int64\n",
       "mother's_occupation                                 int64\n",
       "father's_occupation                                 int64\n",
       "admission_grade                                   float64\n",
       "displaced                                           int64\n",
       "educational_special_needs                           int64\n",
       "debtor                                              int64\n",
       "tuition_fees_up_to_date                             int64\n",
       "gender                                              int64\n",
       "scholarship_holder                                  int64\n",
       "age_at_enrollment                                   int64\n",
       "international                                       int64\n",
       "curricular_units_1st_sem_(credited)                 int64\n",
       "curricular_units_1st_sem_(enrolled)                 int64\n",
       "curricular_units_1st_sem_(evaluations)              int64\n",
       "curricular_units_1st_sem_(approved)                 int64\n",
       "curricular_units_1st_sem_(grade)                  float64\n",
       "curricular_units_1st_sem_(without_evaluations)      int64\n",
       "curricular_units_2nd_sem_(credited)                 int64\n",
       "curricular_units_2nd_sem_(enrolled)                 int64\n",
       "curricular_units_2nd_sem_(evaluations)              int64\n",
       "curricular_units_2nd_sem_(approved)                 int64\n",
       "curricular_units_2nd_sem_(grade)                  float64\n",
       "curricular_units_2nd_sem_(without_evaluations)      int64\n",
       "unemployment_rate                                 float64\n",
       "inflation_rate                                    float64\n",
       "gdp                                               float64\n",
       "target                                             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a6ddf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>Graduate</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Enrolled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>proportion</th>\n",
       "      <td>0.499322</td>\n",
       "      <td>0.321203</td>\n",
       "      <td>0.179476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target      Graduate   Dropout  Enrolled\n",
       "proportion  0.499322  0.321203  0.179476"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(normalize=True).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4327484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = df.drop(columns=['target']).columns\n",
    "# print(len(cols))\n",
    "# num_cols = df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "# print(len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1236ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrmat = df[num_cols].corr()\n",
    "# cm = np.corrcoef(df[num_cols].values.T)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 9))\n",
    "# sns.set_theme(font_scale=1.25)\n",
    "# # sns.heatmap(corrmat, square=True) # vmax = 0.8, cmap='coolwarm'\n",
    "# hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 6}, \n",
    "#                  yticklabels=num_cols, xticklabels=num_cols)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0ef30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566fc32",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4e69653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4424, 36), (4424,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df.drop(columns=['target']), df['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd7df02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3539,), (885,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, shuffle=True, stratify=y)\n",
    "# X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED, shuffle=True, stratify=y_train)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d77257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29563934, -0.0374792 , -0.55645717,  0.31315804,  0.34938119])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_train_scaled = std_scaler.fit_transform(X_train)\n",
    "# X_val_scaled   = std_scaler.transform(X_val)\n",
    "X_test_scaled  = std_scaler.transform(X_test)\n",
    "X_train_scaled[0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a7e3ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Graduate', 'Dropout', 'Dropout', 'Enrolled', 'Dropout'],\n",
       "       dtype=object),\n",
       " array([2, 0, 0, 1, 0]),\n",
       " array(['Dropout', 'Enrolled', 'Graduate'], dtype=object))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train.values)\n",
    "# y_val_enc   = label_encoder.transform(y_val.values)\n",
    "y_test_enc  = label_encoder.transform(y_test.values)\n",
    "y_train[:5].values, y_train_enc[:5], label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7099c0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6d34f",
   "metadata": {},
   "source": [
    "## Модель линейной классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegNumpy():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # --- base params\n",
    "        initial_weights:   list[list[float]] = None, # (n_features, n_classes)\n",
    "        initial_bias:      list[float] = None,       # (1, n_classes)\n",
    "        init_strategy:     Literal['normal', 'corr', 'multistart'] = 'normal',\n",
    "        # --- fit params\n",
    "        total_steps:    int = 1000,\n",
    "        learning_rate:  float = 1e-3,\n",
    "        gd_algo:        Literal['gd', 'sgd', 'minibatch'] = 'sgd',\n",
    "        batch_size:     int | None = None, # None = full dataset\n",
    "        momentum:       float = 0.0,\n",
    "        l2:             float = 0.0,\n",
    "        optim_step:     bool = False,\n",
    "        # early stopping\n",
    "        early_stopping: bool = False,\n",
    "        tolerance:      float = 1e-3,\n",
    "        n_startup_rounds:  int = 10,\n",
    "        early_stop_rounds: int = 5,\n",
    "        validation_fraction: float = 0.1, # (0.0, 1.0)\n",
    "        # recurrent loss function estimation\n",
    "        rec_mode:                Literal['off','mean','ema'] = 'off',\n",
    "        ema_lambda:              float = 0.1,\n",
    "        # sampling strategy\n",
    "        sampling_mode:           Literal['uniform','by_margin'] = 'uniform',\n",
    "        shuffle:                 bool = True,\n",
    "        sampling_tau:            float = 0.2, # TODO: check what it means actually\n",
    "        sampling_min_prob:       float = 0.01,\n",
    "        refresh_rate:            int = 100, # how often to update samples probability distribution\n",
    "        # --- logs\n",
    "        steps_per_epoch:         int | None = 100, # how often to update logs, to evaluate intermediate loss, etc.\n",
    "        verbose:                 bool = False,\n",
    "        # --- misc\n",
    "        use_best_weights:        bool = False,\n",
    "        return_weights_history:  bool = False,\n",
    "        random_seed: int = SEED,\n",
    "        eps: float = 1e-9\n",
    "    ):\n",
    "        # --- base params init\n",
    "        self.weights = (np.array(initial_weights) if initial_weights is not None \n",
    "                        else np.array([]))\n",
    "        self.bias = (np.array(initial_bias) if initial_bias is not None \n",
    "                     else np.array([]))\n",
    "        self.init_strategy = init_strategy\n",
    "        # --- fit params init\n",
    "        self.total_steps = total_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gd_algo = gd_algo\n",
    "        self.batch_size = batch_size\n",
    "        self.momentum = momentum\n",
    "        self.l2 = l2\n",
    "        self.optim_step = optim_step\n",
    "        # early stopping\n",
    "        self.early_stopping = early_stopping\n",
    "        self.tolerance = tolerance\n",
    "        self.n_startup_rounds = n_startup_rounds\n",
    "        self.early_stop_rounds = early_stop_rounds\n",
    "        self.validation_fraction = validation_fraction\n",
    "        # recurrent loss function estimation\n",
    "        self.rec_mode = rec_mode\n",
    "        self.ema_lambda = ema_lambda\n",
    "        # sampling strategy\n",
    "        self.sampling_mode = sampling_mode\n",
    "        self.shuffle = shuffle\n",
    "        self.sampling_tau = sampling_tau\n",
    "        self.sampling_min_prob = sampling_min_prob\n",
    "        self.refresh_rate = refresh_rate\n",
    "        # --- logs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.verbose = verbose\n",
    "        # --- misc init\n",
    "        self.use_best_weights = use_best_weights\n",
    "        self.return_weights_history = return_weights_history\n",
    "        self.random_seed = random_seed\n",
    "        self.eps = eps\n",
    "\n",
    "        self.rng_ = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "        # для рекуррентной оценки\n",
    "        self.rec_value = None\n",
    "        self.rec_count = 0\n",
    "        self.rec_history = []\n",
    "\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        X, y,\n",
    "    ) -> None | list[list[float]]:\n",
    "\n",
    "        input_check = lambda data, dtype: (\n",
    "            np.array(data, dtype=dtype).squeeze()\n",
    "            if not isinstance(data, np.ndarray)\n",
    "            else deepcopy(data).astype(dtype, copy=False)\n",
    "        )\n",
    "        X, y = input_check(X, np.float32), input_check(y, np.int32)\n",
    "        if X.ndim == 1:\n",
    "            X = X[np.newaxis, :]\n",
    "\n",
    "        if self.early_stopping:\n",
    "            X, X_val, y, y_val = train_test_split(\n",
    "                X, y, test_size=self.validation_fraction, \n",
    "                random_state=self.random_seed, shuffle=self.shuffle, stratify=y\n",
    "            )\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # TODO: try python switch-case\n",
    "        if self.gd_algo == 'gd':\n",
    "            self.batch_size = N\n",
    "        elif self.gd_algo == 'sgd':\n",
    "            self.batch_size = 1\n",
    "\n",
    "        if self.steps_per_epoch is None:\n",
    "            self.steps_per_epoch = (N + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        # --- init\n",
    "        # Initialization is done on the whole dataset (sorry for this, but I'm tired)\n",
    "        self._init_weights(X, y, n_starts=5, search_steps=50, lr=self.learning_rate)\n",
    "        if self.return_weights_history:\n",
    "            weights_values = dict(weights=[self.weights.copy()], bias=[self.bias.copy()])\n",
    "        Vdw = np.zeros_like(self.weights)\n",
    "        Vdb = np.zeros_like(self.bias)\n",
    "        self._rec_reset()\n",
    "        self.loss_values = ([], []) if self.early_stopping else [] # Loss is cleared each fit call\n",
    "        rng = self.rng_\n",
    "        learning_rate = self.learning_rate\n",
    "            \n",
    "        def uniform_next_batch_stateful():\n",
    "            perm = rng.permutation(N) if self.shuffle else np.arange(N, dtype=np.int64)\n",
    "            ptr = 0\n",
    "\n",
    "            def get_batch():\n",
    "                nonlocal perm, ptr\n",
    "                if ptr >= N:\n",
    "                    perm = rng.permutation(N) if self.shuffle else np.arange(N, dtype=np.int64)\n",
    "                    ptr = 0\n",
    "\n",
    "                remaining = N - ptr\n",
    "                take = self.batch_size if self.batch_size <= remaining else remaining\n",
    "\n",
    "                idx = perm[ptr:ptr + take]\n",
    "                ptr += take\n",
    "                return idx\n",
    "\n",
    "            return get_batch\n",
    "\n",
    "        margin_probs = lambda: self._margin_sampling_probs(\n",
    "            X, y, use_abs=True, tau=self.sampling_tau, min_prob=self.sampling_min_prob\n",
    "        )\n",
    "\n",
    "        # ---- training loop (one unified path)\n",
    "        step = 0\n",
    "        block_loss_sum = 0.0\n",
    "        block_count = 0\n",
    "\n",
    "        if self.sampling_mode == 'uniform':\n",
    "            next_uniform_batch = uniform_next_batch_stateful()\n",
    "        elif self.sampling_mode == 'by_margin':\n",
    "            probs = margin_probs()\n",
    "            next_uniform_batch = None\n",
    "        else:\n",
    "            raise ValueError(\"sampling_mode must be 'uniform' or 'by_margin'\")\n",
    "\n",
    "        # --- init validation score\n",
    "        # Train\n",
    "        loss = self._get_loss(X, y)\n",
    "        if self.early_stopping:\n",
    "            self.loss_values[0].append(loss)\n",
    "        else:\n",
    "            self.loss_values.append(loss)\n",
    "        # recurrent quality update (train loss)\n",
    "        train_rec_val = self._rec_update(loss)\n",
    "        self.rec_history.append(train_rec_val)\n",
    "        # Validation\n",
    "        if self.early_stopping:\n",
    "            loss = self._get_loss(X_val, y_val)\n",
    "            self.loss_values[1].append(loss)\n",
    "\n",
    "        # for early stopping similarly to sklearn\n",
    "        no_improvement_counter = 0   \n",
    "        best_loss = loss\n",
    "        best_step = 0\n",
    "        self.t = []\n",
    "\n",
    "        while step < self.total_steps:\n",
    "            if self.sampling_mode == 'uniform':\n",
    "                batch_idx = next_uniform_batch()\n",
    "            else:  # by_margin\n",
    "                if step % self.refresh_rate == 0:\n",
    "                    probs = margin_probs()\n",
    "                batch_idx = rng.choice(N, size=self.batch_size, replace=True, p=probs)\n",
    "\n",
    "            xi = X[batch_idx, :]\n",
    "            yi = y[batch_idx]\n",
    "\n",
    "            # forward / loss\n",
    "            logits = self.forward(xi)\n",
    "            loss   = self._loss_fn_opt(yi, logits, reduction=None)\n",
    "\n",
    "            # Self-Normalized Importance Sampling Loss\n",
    "            if self.sampling_mode == 'by_margin':\n",
    "                pi = probs[batch_idx]\n",
    "                sample_weights = 1.0 / np.clip(pi, 1e-12, None)\n",
    "                loss = (loss * sample_weights).sum() / sample_weights.sum()\n",
    "            else:\n",
    "                sample_weights = None\n",
    "                loss = loss.mean()\n",
    "            \n",
    "            block_loss_sum += loss\n",
    "            block_count    += 1\n",
    "\n",
    "            # recurrent quality update (train loss)\n",
    "            train_rec_val = self._rec_update(loss)\n",
    "            self.rec_history.append(train_rec_val)\n",
    "\n",
    "            # gradients\n",
    "            w_grad, b_grad = self._gradient(xi, yi, logits)\n",
    "            # L2 - regularization\n",
    "            if self.l2 > 0.0:\n",
    "                w_grad += self.l2 * self.weights\n",
    "\n",
    "            # momentum (EMA style)\n",
    "            Vdw = self.momentum * Vdw - (1.0 - self.momentum) * w_grad\n",
    "            Vdb = self.momentum * Vdb - (1.0 - self.momentum) * b_grad\n",
    "            \n",
    "            if self.optim_step:\n",
    "                # learning_rate = self._line_search_backtracking(\n",
    "                #     xi, yi, w_grad, b_grad, Vdw, Vdb\n",
    "                # )\n",
    "                learning_rate = self._golden_ratio_search(\n",
    "                    xi, yi, Vdw, Vdb, 0, 1, 1e-5, 1000\n",
    "                )\n",
    "                self.t.append(learning_rate)\n",
    "\n",
    "            # update\n",
    "            self.weights += learning_rate * Vdw\n",
    "            self.bias    += learning_rate * Vdb\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            # Logging once per “epoch-sized” number of steps.\n",
    "            # Early stopping criteria check\n",
    "            if block_count >= self.steps_per_epoch:\n",
    "                mean_block_loss = block_loss_sum / block_count\n",
    "                if self.early_stopping:\n",
    "                    self.loss_values[0].append(mean_block_loss)\n",
    "                    loss = self._get_loss(X_val, y_val)\n",
    "                    self.loss_values[1].append(loss)\n",
    "                else:\n",
    "                    self.loss_values.append(mean_block_loss)\n",
    "                    # Workaround, loss value should represent global tendency in loss change.\n",
    "                    # Current loss is value for a single opt. step.\n",
    "                    loss = mean_block_loss\n",
    "\n",
    "                # # recurrent quality update - for better plots maybe should log once per block, not every opt. step\n",
    "                # rec_val = self._rec_update(mean_block_loss, mode=rec_mode, ema_lambda=ema_lambda)\n",
    "                # self.rec_history.append(rec_val)\n",
    "\n",
    "                # Early stopping on monitored series (smoothed if rec_mode != 'off').\n",
    "                # Done on training set, if early_stopping = False, else on validation set\n",
    "                # Recalculating loss on validation each opt. step is too costly actually\n",
    "                if (self.tolerance is not None) and (step > self.n_startup_rounds + 1):\n",
    "                    if loss > best_loss - self.tolerance:\n",
    "                        no_improvement_counter += 1\n",
    "                        if no_improvement_counter >= self.early_stop_rounds:\n",
    "                            # Check if last loss still a bit smaller than best\n",
    "                            if loss < best_loss:\n",
    "                                best_loss = loss\n",
    "                            if self.verbose:\n",
    "                                print(f\"Last loss : {loss}\\t Best loss: {best_loss} on step {best_step}({best_step // self.steps_per_epoch})\\t\")\n",
    "                                print(f\"Early stopping at step {step}({step // self.steps_per_epoch})\")\n",
    "                            break\n",
    "                    else:\n",
    "                        best_loss = loss\n",
    "                        no_improvement_counter = 0\n",
    "                        best_step = step\n",
    "                        if self.use_best_weights:\n",
    "                            best_weights = self.weights.copy()\n",
    "                            best_bias = self.bias.copy()\n",
    "                        \n",
    "\n",
    "\n",
    "                if self.return_weights_history:\n",
    "                    weights_values['weights'].append(self.weights.copy())\n",
    "                    weights_values['bias'].append(self.bias.copy())\n",
    "                if self.verbose:\n",
    "                    print(\n",
    "                        f\"step {step:6d} ({step // self.steps_per_epoch}) \"\n",
    "                        f\"| block_loss={mean_block_loss:.6f} \"\n",
    "                        f\"| val_loss={loss:.6f} \"\n",
    "                        f\"| batch_size={self.batch_size} \"\n",
    "                        f\"| mode={self.sampling_mode}\"\n",
    "                    )\n",
    "\n",
    "                block_loss_sum = 0.0\n",
    "                block_count = 0\n",
    "\n",
    "        if self.use_best_weights:\n",
    "            self.weights = best_weights\n",
    "            self.bias = best_bias\n",
    "\n",
    "        if self.return_weights_history:\n",
    "            return np.array(weights_values)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def _get_loss(self, X, y):\n",
    "        logits = self.forward(X)\n",
    "        loss = self._loss_fn_opt(y, logits, reduction='mean')\n",
    "        return loss\n",
    "\n",
    "    def _golden_ratio_search(\n",
    "        self,\n",
    "        X, y,\n",
    "        dir_w:  np.ndarray, dir_b:  np.ndarray,\n",
    "        lo: float = 0.0, hi: float = 1.0,\n",
    "        tol: float = 1e-6, max_iters: int = 1000\n",
    "    ):  \n",
    "        # zero iteration\n",
    "        logits_0 = self.forward(X)\n",
    "        Xdw = X @ dir_w\n",
    "        Wdw = np.sum(self.weights * dir_w)\n",
    "        dw_norm2 = np.sum(dir_w * dir_w)\n",
    "        ######################################\n",
    "\n",
    "        def f(t):\n",
    "            logits_t = logits_0 + t * (Xdw + dir_b)\n",
    "            loss_t = self._loss_fn_opt(y, logits_t, reduction='mean')\n",
    "            if self.l2 > 0.0:\n",
    "                loss_t += self.l2 * 0.5 * (2.0 * t * Wdw + t * t * dw_norm2)\n",
    "            return loss_t\n",
    "        \n",
    "        inv_phi = (5**0.5 - 1) / 2\n",
    "        x1 = hi - inv_phi * (hi - lo)\n",
    "        x2 = lo + inv_phi * (hi - lo)\n",
    "        f1 = f(x1)\n",
    "        f2 = f(x2)\n",
    "\n",
    "        i = 0\n",
    "        while (hi - lo) / 2 >= tol and i < max_iters:            \n",
    "\n",
    "            if f1 > f2:\n",
    "                lo, f1 = x1, f2\n",
    "                x1 = hi - inv_phi * (hi - lo)\n",
    "                x2 = lo + inv_phi * (hi - lo)\n",
    "                f2 = f(x2)\n",
    "            else:\n",
    "                hi, f2 = x2, f1\n",
    "                x1 = hi - inv_phi * (hi - lo)\n",
    "                x2 = lo + inv_phi * (hi - lo)\n",
    "                f1 = f(x1)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "        if i == max_iters:\n",
    "            print(f'Optimal search max iter reached, step is unoptimal')\n",
    "\n",
    "        return (lo + hi) / 2.\n",
    "        \n",
    "\n",
    "    # Boyd & Vandenberg 464-...\n",
    "    def _line_search_backtracking(\n",
    "        self,\n",
    "        X, y, \n",
    "        grad_w: np.ndarray, grad_b: np.ndarray, \n",
    "        dir_w:  np.ndarray, dir_b:  np.ndarray,\n",
    "        step: float = 1.0, \n",
    "        alpha: float = 1e-3, \n",
    "        beta: float = 0.5,\n",
    "        gamma: float = 0.85,\n",
    "        tol: float = 1e-8,\n",
    "        default_lr: float = 1e-9,\n",
    "        eps: float = 1e-12\n",
    "    ):\n",
    "        logits_0 = self.forward(X)\n",
    "        loss_0 = self._loss_fn_opt(y, logits_0, reduction='mean')\n",
    "        if self.l2 > 0.0:\n",
    "            loss_0 += self.l2 * np.sum(np.pow(self.weights, 2)) * 0.5\n",
    "        \n",
    "        W0 = self.weights\n",
    "        b0 = self.bias  \n",
    "\n",
    "        # directional derivatives\n",
    "        dw = (grad_w * dir_w).sum() # already **squared** L2-norm\n",
    "        db = (grad_b * dir_b).sum()\n",
    "        dd = dw + db\n",
    "\n",
    "        if dd >= -eps:\n",
    "            # switching to regular gradient descent\n",
    "            gnorm2 = (grad_w * grad_w).sum() + (grad_b * grad_b).sum()\n",
    "            if gnorm2 <= eps:\n",
    "                return 0.0\n",
    "            print('descent dir > 0, switching to gradient descent')\n",
    "            dd = -gnorm2\n",
    "            np.copyto(dst=dir_w, src=-grad_w); np.copyto(dst=dir_b, src=-grad_b)\n",
    "\n",
    "        t = step\n",
    "\n",
    "        # TODO: добавить оптимизации через раскрытие logits_t и l2_norm и предварительного подсчета неизменных членов\n",
    "        while t > tol:\n",
    "            Wt = W0 + t * dir_w\n",
    "            bt = b0 + t * dir_b\n",
    "            logits_t = np.matmul(X, Wt) + bt\n",
    "            loss_t = self._loss_fn_opt(y, logits_t, reduction='mean')\n",
    "            if self.l2 > 0.0:\n",
    "                loss_t += self.l2 * np.sum(np.pow(Wt, 2)) * 0.5\n",
    "            \n",
    "            wolfe_cond = (\n",
    "                loss_t <= loss_0 + alpha * t * dd\n",
    "                and np.abs(dd * loss_t) <= gamma * np.abs(dd * loss_0)\n",
    "            )\n",
    "            if wolfe_cond:\n",
    "                return t\n",
    "            \n",
    "            t *= beta\n",
    "\n",
    "        return t\n",
    "\n",
    "\n",
    "    def predict(self, features: list[list[float]]):\n",
    "        X = (np.array(features).squeeze() if not isinstance(features, np.ndarray) \n",
    "             else deepcopy(features).astype(np.float32, copy=False))\n",
    "        if X.ndim == 1:\n",
    "            X = X[np.newaxis, :]\n",
    "        logits = self.forward(X) # (n_samples, n_classes)\n",
    "        probs  = self._softmax(logits) # не обязательно\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def predict_proba(self, features: list[list[float]]):\n",
    "        X = (np.array(features).squeeze() if not isinstance(features, np.ndarray) \n",
    "             else deepcopy(features).astype(np.float32, copy=False))\n",
    "        if X.ndim == 1:\n",
    "            X = X[np.newaxis, :]\n",
    "        logits = self.forward(X) # (n_samples, n_classes)\n",
    "        probs  = self._softmax(logits) # не обязательно\n",
    "        return probs\n",
    "    \n",
    "    def _create_onehot_target(self, y: np.array):\n",
    "        ohe_enc = OneHotEncoder(categories=[np.unique(y)], sparse_output=False)\n",
    "        y_enc = ohe_enc.fit_transform(y.reshape(-1, 1))\n",
    "        return y_enc # output -> (n_samples, n_classes)\n",
    "    \n",
    "    def _init_weights(\n",
    "        self, X: np.ndarray, y: np.ndarray,\n",
    "        n_starts: int = 5, search_steps: int = 50, lr: float = 1e-2,\n",
    "    ):\n",
    "        N, d = X.shape\n",
    "        K = np.max(y) + 1\n",
    "\n",
    "        if self.init_strategy == 'normal':\n",
    "            if self.weights.size == 0:\n",
    "                self.weights = self.rng_.standard_normal((d, K), dtype=np.float32)\n",
    "            if self.bias.size == 0:\n",
    "                self.bias = self.rng_.standard_normal((1, K), dtype=np.float32)\n",
    "            return\n",
    "\n",
    "        if self.init_strategy == 'corr':\n",
    "            # by default this strategy assumes that input data already was standardized\n",
    "            if self.weights.size != 0 and self.bias.size != 0:\n",
    "                return\n",
    "            # евклидова норма\n",
    "            # denom = np.sum(X * X, axis=0)       # shape (d,)\n",
    "            denom = np.float64(N)\n",
    "\n",
    "            W = np.zeros((d, K), dtype=np.float64)\n",
    "            b = np.zeros((1, K), dtype=np.float64)\n",
    "\n",
    "            for k in range(K):\n",
    "                t = (y == k).astype(np.float64) # 1 for class k, else 0\n",
    "                # weights: elementwise division by per-feature squared norm\n",
    "                # With centered X, X^T t == X^T (t - mean(t)), so no need to center t explicitly.\n",
    "                numer = X.T @ t                           # shape (d,)\n",
    "                W[:, k] = numer / denom\n",
    "\n",
    "                # intercept: with centered features, LS gives b_k = mean(t^{(k)})\n",
    "                b[0, k] = t.mean()\n",
    "\n",
    "            if self.weights.size == 0:\n",
    "                self.weights = W.astype(np.float32, copy=False)\n",
    "            if self.bias.size == 0:\n",
    "                self.bias = b.astype(np.float32, copy=False)\n",
    "            return\n",
    "        \n",
    "        if self.init_strategy == 'multistart':\n",
    "            best_loss = np.inf\n",
    "            best_W, best_b = None, None\n",
    "\n",
    "            for _ in range(n_starts):\n",
    "                W = self.rng_.standard_normal((d, K), dtype=np.float32)\n",
    "                b = self.rng_.standard_normal((1, K), dtype=np.float32)\n",
    "                \n",
    "                # short warmup\n",
    "                w, b, loss = self._warmup(X, y, W, b, steps=search_steps, lr=lr)\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_W, best_b = w, b\n",
    "\n",
    "                self.weights = best_W\n",
    "                self.bias    = best_b\n",
    "            return\n",
    "        \n",
    "        raise ValueError(\"init_strategy must be 'normal' or 'corr'\")\n",
    "    \n",
    "    # Warmup is performed on the whole input dataset, not on batches\n",
    "    def _warmup(self, X, y, W, b, steps=50, lr=1e-2):\n",
    "        W = W.copy(); b = b.copy()\n",
    "        for _ in range(steps):\n",
    "            logits = np.matmul(X, W) + b\n",
    "            loss   = self._loss_fn_opt(y, logits, reduction='mean')\n",
    "            w_grad, b_grad = self._gradient(X, y, logits)\n",
    "            W -= lr * w_grad\n",
    "            b -= lr * b_grad\n",
    "        return W, b, float(loss)\n",
    "    \n",
    "        \n",
    "    def _softmax(self, X: np.array) -> np.array:\n",
    "        Z = X - np.max(X, axis=1, keepdims=True)\n",
    "        numerator = np.exp(Z)\n",
    "        denominator = np.sum(numerator, axis=1, keepdims=True)\n",
    "        softmax_probs = numerator / denominator\n",
    "        return softmax_probs # -> (n_samples, n_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # (n_samples, n_features) * (n_features, n_classes)\n",
    "        logits = np.matmul(X, self.weights) + self.bias # -> (n_samples, n_classes)\n",
    "        return logits\n",
    "    \n",
    "    # def loss_fn_expanded(self, X, y_true): # КАК Я ЭТО ВЫВЕЛ???\n",
    "    #     # (n_samples, n_features) * (n_features, n_classes) + (n_samples, 1) * (1, n_classes) = (n_samples, n_classes)\n",
    "    #     logits = np.matmul(X, self.weights) + np.matmul(np.ones((X.shape[0], 1)), self.bias)\n",
    "    #     exp_logits = np.exp(logits)\n",
    "    #     logits_sum = np.sum(exp_logits, axis=1) # -> (n_samples, 1)\n",
    "    #     # (n_samples, n_classes) * (n_samples, n_classes)\n",
    "    #     true_class_logits = logits[np.arange(X.shape[0]), y_true] \n",
    "    #     return np.mean(np.log(logits_sum) - true_class_logits)\n",
    "\n",
    "    # def loss_fn(self, y_true, logits): # наивная реализация по ф-ле с доки pytorch\n",
    "    #     log_probs = np.log(self.softmax(logits)) # -> (n_samples, classes)\n",
    "    #     # y_true_ohe = self.create_onehot_target(y_true) # -> (n_samples, classes)\n",
    "    #     # likelihood = (log_probs * y_true_ohe).sum(axis=1).mean()\n",
    "    #     likelihood = (log_probs[np.arange(log_probs.shape[0]), y_true]).mean()\n",
    "    #     return -likelihood\n",
    "    \n",
    "    def _loss_fn_opt(self, y_true, logits, reduction=None):\n",
    "        lse = logsumexp(logits, axis=1, keepdims=True)\n",
    "        nll = lse - logits\n",
    "        loss = nll[np.arange(nll.shape[0]), y_true]\n",
    "        if reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "    \n",
    "    def _rec_reset(self):\n",
    "        self.rec_value = None\n",
    "        self.rec_count = 0\n",
    "        self.rec_history = []\n",
    "\n",
    "    def _rec_update(self, xi):\n",
    "        # Smoothing is applied only to train loss history.\n",
    "        # Validation loss shouldn't be smoothed.\n",
    "        # One possible explanation for this - validation loss is always calculated on the whole val set,\n",
    "        # so there is no more reason to apply smoothing to it. \n",
    "        if self.rec_mode == \"off\":\n",
    "            return xi\n",
    "\n",
    "        if self.rec_value is None:\n",
    "            # инициализация последовательности\n",
    "            self.rec_value = xi\n",
    "            self.rec_count = 1\n",
    "            return self.rec_value\n",
    "\n",
    "        if self.rec_mode == \"mean\":\n",
    "            # running mean: Q_m = (1/m)*xi_m + (1 - 1/m)*Q_{m-1}\n",
    "            self.rec_count += 1\n",
    "            m = self.rec_count\n",
    "            self.rec_value = (1.0 / m) * xi + (1.0 - 1.0 / m) * self.rec_value\n",
    "            return self.rec_value\n",
    "\n",
    "        if self.rec_mode == \"ema\":\n",
    "            # EMA: Q_m = λ xi_m + (1 - λ) Q_{m-1}\n",
    "            self.rec_value = self.ema_lambda * xi + (1.0 - self.ema_lambda) * self.rec_value\n",
    "            return self.rec_value\n",
    "\n",
    "        return xi\n",
    "\n",
    "    def _gradient(self, X, y_true, logits):\n",
    "        y_prob = self._softmax(logits)\n",
    "        y_prob[np.arange(y_prob.shape[0]), y_true] -= 1\n",
    "        y_prob /= y_prob.shape[0]\n",
    "        w_grad = np.matmul(X.T, y_prob)\n",
    "        b_grad = y_prob.sum(axis=0, keepdims=True)\n",
    "        return w_grad, b_grad\n",
    "    \n",
    "    # Only for multiclass, no separate implementation for binary case\n",
    "    # TODO: make binary case margin estimation\n",
    "    def calc_margins(self, X, y_true):\n",
    "        logits = self.forward(X)\n",
    "        true_logits = logits[np.arange(X.shape[0]), y_true]\n",
    "        logits[np.arange(logits.shape[0]), y_true] = -np.inf\n",
    "        false_logits = logits.max(axis=1)\n",
    "        margins = true_logits - false_logits\n",
    "        return margins\n",
    "\n",
    "    # TODO: сделать сэмплер с разной логикой выбора сложных случаев\n",
    "    # 1) -abs(margins) - для любых (правильных или нет) случаев с малой долей уверенности\n",
    "    # 2) -margins вместо -abs(margins) - для точно неправильно классифицированных случаев\n",
    "    def _margin_sampling_probs(\n",
    "        self, X, y, use_abs: bool = True, tau: float = 0.2, min_prob: float = 0.01\n",
    "    ):\n",
    "        margins = self.calc_margins(X, y)\n",
    "\n",
    "        diff = -np.abs(margins) if use_abs else -margins\n",
    "        scores = diff / max(tau, 1e-8)\n",
    "        probs = self._softmax(scores.reshape(1, -1)).squeeze()\n",
    "\n",
    "        floor = min_prob / X.shape[0]\n",
    "        probs = (1.0 - min_prob) * probs + floor\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e6d8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogRegNumpy(\n",
    "    # --- base params\n",
    "    init_strategy = 'normal',\n",
    "    # --- fit args\n",
    "    total_steps = X_train_scaled.shape[0]*100, \n",
    "    learning_rate = 1e-2,\n",
    "    gd_algo = 'sgd',\n",
    "    batch_size = 32,\n",
    "    momentum = 0.0,\n",
    "    l2 = 0.01, \n",
    "    optim_step = True,\n",
    "    # early stopping\n",
    "    early_stopping = True,\n",
    "    tolerance = 1e-3,\n",
    "    n_startup_rounds = 0,\n",
    "    early_stop_rounds = 5,\n",
    "    validation_fraction = 0.1,\n",
    "    # рекуррентная оценка функции потерь\n",
    "    rec_mode = 'ema', \n",
    "    ema_lambda = 0.01,\n",
    "    # стратегия сэмплирования\n",
    "    sampling_mode = 'uniform',\n",
    "    shuffle = True, \n",
    "    sampling_tau = 0.5,\n",
    "    sampling_min_prob = 0.01, \n",
    "    refresh_rate = 200,\n",
    "    # --- logs\n",
    "    steps_per_epoch = 500,\n",
    "    verbose=True,\n",
    "    # --- misc\n",
    "    use_best_weights = False,\n",
    "    return_weights_history = False,\n",
    "    random_seed = SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bb3aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params = params_json['LogRegNumpy']['SGD_optimal_step']\n",
    "# model_params['verbose'] = True\n",
    "# model_params['early_stop_rounds'] = 15\n",
    "# logreg = LogRegNumpy(\n",
    "#     **model_params,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4636f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    500 (1) | block_loss=1.809410 | val_loss=1.757960 | batch_size=1 | mode=uniform\n",
      "step   1000 (2) | block_loss=2.034574 | val_loss=1.809523 | batch_size=1 | mode=uniform\n",
      "step   1500 (3) | block_loss=1.952634 | val_loss=2.019309 | batch_size=1 | mode=uniform\n",
      "step   2000 (4) | block_loss=1.671800 | val_loss=2.418669 | batch_size=1 | mode=uniform\n",
      "step   2500 (5) | block_loss=2.005345 | val_loss=1.624929 | batch_size=1 | mode=uniform\n",
      "step   3000 (6) | block_loss=1.477650 | val_loss=2.083903 | batch_size=1 | mode=uniform\n",
      "step   3500 (7) | block_loss=1.807248 | val_loss=1.571751 | batch_size=1 | mode=uniform\n",
      "step   4000 (8) | block_loss=1.819127 | val_loss=1.588921 | batch_size=1 | mode=uniform\n",
      "step   4500 (9) | block_loss=1.591933 | val_loss=1.622998 | batch_size=1 | mode=uniform\n",
      "step   5000 (10) | block_loss=1.729460 | val_loss=1.729807 | batch_size=1 | mode=uniform\n",
      "step   5500 (11) | block_loss=1.754657 | val_loss=1.561346 | batch_size=1 | mode=uniform\n",
      "step   6000 (12) | block_loss=2.004644 | val_loss=2.131000 | batch_size=1 | mode=uniform\n",
      "step   6500 (13) | block_loss=2.101702 | val_loss=1.676970 | batch_size=1 | mode=uniform\n",
      "step   7000 (14) | block_loss=2.013825 | val_loss=1.609220 | batch_size=1 | mode=uniform\n",
      "step   7500 (15) | block_loss=1.809677 | val_loss=1.236102 | batch_size=1 | mode=uniform\n",
      "step   8000 (16) | block_loss=2.052055 | val_loss=2.544535 | batch_size=1 | mode=uniform\n",
      "step   8500 (17) | block_loss=1.643652 | val_loss=1.922717 | batch_size=1 | mode=uniform\n",
      "step   9000 (18) | block_loss=1.861711 | val_loss=1.530141 | batch_size=1 | mode=uniform\n",
      "step   9500 (19) | block_loss=1.508037 | val_loss=3.113287 | batch_size=1 | mode=uniform\n",
      "Last loss : 1.8733755350112915\t Best loss: 1.2361016273498535 on step 7500(15)\t\n",
      "Early stopping at step 10000(20)\n"
     ]
    }
   ],
   "source": [
    "logreg.fit(X_train_scaled, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34c2a5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01839638, 0.02146711, 0.02167508, ..., 0.99997659, 0.99998255,\n",
       "        0.9999922 ]),\n",
       " array([   1,    1,    1, ...,   30,   45, 4824]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(logreg.t, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d0c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6), sharey='row')\n",
    "\n",
    "ax[0].spines[['right', 'top']].set_visible(False)\n",
    "ax[0].grid(ls='--', alpha=0.6)\n",
    "ax[0].plot(logreg.loss_values[0], label='train')\n",
    "ax[0].plot(logreg.loss_values[1], label='test')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title(\"Train vs Test loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].spines[['right', 'top']].set_visible(False)\n",
    "ax[1].grid(ls='--', alpha=0.6)\n",
    "ax[1].plot(logreg.rec_history)\n",
    "ax[1].set_xlabel('Step')\n",
    "ax[1].set_ylabel('Loss (smoothed)')\n",
    "ax[1].set_title(\"Train (smoothed) loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfaad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(ls='--', alpha=0.6)\n",
    "plt.plot(logreg.loss_values[0], label='train')\n",
    "plt.plot(logreg.loss_values[1], label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Train vs Test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.grid(ls='--', alpha=0.6)\n",
    "plt.plot(logreg.rec_history)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss (smoothed)')\n",
    "plt.title(\"Train vs Test (smoothed) loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_margins(margins, eps=1.0, display_plot: bool = False, save_path: str = ''):\n",
    "    if save_path:\n",
    "        save_path = Path(save_path)\n",
    "\n",
    "    sorted_idx = np.argsort(margins)\n",
    "    sorted_margins = margins[sorted_idx]\n",
    "    \n",
    "    line_kwargs      = {'lw': 2}\n",
    "    pos_fill_kwargs  = {'alpha': 0.25, 'color': 'tab:green'}\n",
    "    neg_fill_kwargs  = {'alpha': 0.25, 'color': 'tab:red'}\n",
    "    zero_fill_kwargs = {'alpha': 0.25, 'color': 'gold'}\n",
    "\n",
    "    # masks\n",
    "    if eps > 0.0:\n",
    "        mask_zero = np.abs(sorted_margins) <= eps\n",
    "        mask_pos  = sorted_margins >  eps\n",
    "        mask_neg  = sorted_margins < -eps\n",
    "    else:\n",
    "        mask_zero = np.zeros_like(sorted_margins, dtype=bool)\n",
    "        mask_pos  = sorted_margins > 0\n",
    "        mask_neg  = sorted_margins < 0\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    # line\n",
    "    plot_idx = np.arange(sorted_margins.shape[0])\n",
    "    plt.plot(plot_idx, sorted_margins, **line_kwargs)\n",
    "    plt.axhline(0.0, color='black', lw=1, alpha=0.7)\n",
    "\n",
    "    if np.any(mask_neg):\n",
    "        plt.fill_between(plot_idx, sorted_margins, 0.0, where=mask_neg, interpolate=True, **neg_fill_kwargs)\n",
    "    if np.any(mask_zero):\n",
    "        plt.fill_between(plot_idx, sorted_margins, 0.0, where=mask_zero, interpolate=True, **zero_fill_kwargs)\n",
    "    if np.any(mask_pos):\n",
    "        plt.fill_between(plot_idx, sorted_margins, 0.0, where=mask_pos, interpolate=True, **pos_fill_kwargs)\n",
    "\n",
    "    plt.xlabel(\"sample index (sorted)\")\n",
    "    plt.ylabel(\"margin\")\n",
    "    plt.title(\"Margin curve with signed areas\")\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, format='png')\n",
    "    if display_plot:\n",
    "        plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "margins_train = logreg.calc_margins(X_train_scaled, y_train_enc)\n",
    "margins_test  = logreg.calc_margins(X_test_scaled,  y_test_enc)\n",
    "\n",
    "viz_margins(margins_train, display_plot=True)\n",
    "viz_margins(margins_test, display_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7471460",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = logreg.predict(X_train_scaled)\n",
    "# preds_val   = logreg.predict(X_val_scaled)\n",
    "preds_test  = logreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93eb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train accuracy: {(preds_train == y_train_enc).mean()*100.:.4f}')\n",
    "# print(f'Validation accuracy: {(preds_val == y_val_enc).mean()*100.:.4f}')\n",
    "print(f'Test accuracy: {(preds_test == y_test_enc).mean()*100.:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- your existing helpers kept as-is ----\n",
    "def _ensure_class_names(y_true, y_pred, class_names):\n",
    "    if class_names is None:\n",
    "        classes = sorted(list(set(np.asarray(y_true).tolist()) | set(np.asarray(y_pred).tolist())))\n",
    "        class_names = [str(c) for c in classes]\n",
    "        labels = classes\n",
    "    else:\n",
    "        labels = class_names if not np.issubdtype(np.asarray(y_true).dtype, np.integer) else list(range(len(class_names)))\n",
    "    return class_names, labels\n",
    "\n",
    "def plot_multiclass_confusion_matrix(\n",
    "    y_true, y_pred, class_names=None, normalize=True, cmap=\"Blues\", figsize=(10,8), annot=True, title=None\n",
    "):\n",
    "    class_names, labels = _ensure_class_names(y_true, y_pred, class_names)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    if normalize:\n",
    "        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "            cm = cm / cm.sum(axis=1, keepdims=True)\n",
    "        fmt, cbar_label = \".2f\", \"Proportion\"\n",
    "    else:\n",
    "        fmt, cbar_label = \"d\", \"Count\"\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.heatmap(cm, annot=annot, fmt=fmt, cmap=cmap,\n",
    "                     xticklabels=class_names, yticklabels=class_names,\n",
    "                     linewidths=.5, linecolor=\"white\", cbar_kws={\"label\": cbar_label})\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(title or (\"Confusion Matrix (row-normalized)\" if normalize else \"Confusion Matrix\"))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def per_class_tp_fp_fn_tn_table(y_true, y_pred, class_names=None):\n",
    "    class_names, labels = _ensure_class_names(y_true, y_pred, class_names)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    total = cm.sum()\n",
    "    tp = np.diag(cm)\n",
    "    fp = cm.sum(axis=0) - tp\n",
    "    fn = cm.sum(axis=1) - tp\n",
    "    tn = total - (tp + fp + fn)\n",
    "    return pd.DataFrame({\"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn}, index=class_names)\n",
    "\n",
    "def plot_tp_fp_fn_tn_table(y_true, y_pred, class_names=None, cmap=\"YlGnBu\", figsize=(6,1.2), title=None):\n",
    "    df = per_class_tp_fp_fn_tn_table(y_true, y_pred, class_names)\n",
    "    plt.figure(figsize=(figsize[0], figsize[1] * len(df)))\n",
    "    ax = sns.heatmap(df, annot=True, fmt=\"d\", cmap=cmap, cbar=False,\n",
    "                     linewidths=.5, linecolor=\"white\")\n",
    "    ax.set_xlabel(\"Metric\")\n",
    "    ax.set_ylabel(\"Class\")\n",
    "    ax.set_title(title or \"Per-class TP / FP / FN / TN\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ----------------- model adapters you already had -----------------\n",
    "def _try_fit(model, Xtr, ytr, fit_args, Xval=None, yval=None):\n",
    "    try:\n",
    "        return model.fit(Xtr, ytr, **(fit_args or {}))\n",
    "    except TypeError:\n",
    "        val_set = (Xval, yval) if (Xval is not None and yval is not None) else (None, None)\n",
    "        return model.fit((Xtr, ytr), val_set, **(fit_args or {}))\n",
    "\n",
    "def _proba(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)\n",
    "    df = model.decision_function(X)\n",
    "    if df.ndim == 1:\n",
    "        df = np.c_[-df, df]\n",
    "    e = np.exp(df - df.max(axis=1, keepdims=True))\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "# ----------------- New pretty table utilities (alignment-fixed, ASCII fallback) -----------------\n",
    "\n",
    "def _print_cv_tables(cv_df: pd.DataFrame, *, title=\"Cross-Validation\", ascii_borders=True):\n",
    "    cv_df = cv_df.copy()\n",
    "    cv_df.index.name = cv_df.index.name or \"fold\"\n",
    "\n",
    "    # numeric-only mean row (avoids dtype warnings)\n",
    "    mean_row = cv_df.mean(numeric_only=True)\n",
    "    combined = cv_df.copy()\n",
    "    combined.loc[\"mean\"] = mean_row\n",
    "\n",
    "    # draw a rule after the last fold row\n",
    "    split_after = [len(cv_df) - 1]\n",
    "\n",
    "    _print_box_table(\n",
    "        combined,\n",
    "        title=title,\n",
    "        index_name=cv_df.index.name,\n",
    "        ascii_borders=ascii_borders,\n",
    "        split_after_rows=split_after,\n",
    "    )\n",
    "\n",
    "\n",
    "def _print_box_table(\n",
    "    df: pd.DataFrame,\n",
    "    title=None,\n",
    "    digits=4,\n",
    "    split_after_rows=None,\n",
    "    index_name=None,\n",
    "    ascii_borders=False,   # <- toggle if your console mis-renders box characters\n",
    "):\n",
    "    \"\"\"\n",
    "    Pretty box table with:\n",
    "      - numeric headers & values right-aligned\n",
    "      - index header & values left-aligned\n",
    "      - robust width calculation\n",
    "      - optional ASCII borders if Unicode box-drawing looks misaligned\n",
    "    \"\"\"\n",
    "    if title:\n",
    "        print(\"\\n\" + title)\n",
    "        print(\"-\" * len(title))\n",
    "\n",
    "    # choose border chars\n",
    "    if ascii_borders:\n",
    "        TL, TM, TR = \"+\", \"+\", \"+\"\n",
    "        ML, MM, MR = \"+\", \"+\", \"+\"\n",
    "        BL, BM, BR = \"+\", \"+\", \"+\"\n",
    "        V = \"|\"\n",
    "        H = \"-\"\n",
    "    else:\n",
    "        TL, TM, TR = \"┌\", \"┬\", \"┐\"\n",
    "        ML, MM, MR = \"├\", \"┼\", \"┤\"\n",
    "        BL, BM, BR = \"└\", \"┴\", \"┘\"\n",
    "        V = \"│\"\n",
    "        H = \"─\"\n",
    "\n",
    "    def _is_num(x):\n",
    "        return isinstance(x, (int, np.integer, float, np.floating)) or (isinstance(x, str) and x.replace(\".\",\"\",1).isdigit())\n",
    "\n",
    "    def _fmt_cell(x):\n",
    "        if isinstance(x, (float, np.floating)):\n",
    "            return f\"{x:.{digits}f}\"\n",
    "        if isinstance(x, (int, np.integer)):\n",
    "            return str(int(x))\n",
    "        if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "            return \"\"\n",
    "        return str(x)\n",
    "\n",
    "    # copy as object so we fully control string formatting\n",
    "    df = df.copy()\n",
    "    df = df.astype(object)\n",
    "\n",
    "    # per-column \"numericness\": treat a column numeric if ALL non-blank entries are numeric\n",
    "    is_num_col = {}\n",
    "    for c in df.columns:\n",
    "        col = df[c].map(lambda v: (v is None) or (isinstance(v, float) and np.isnan(v)) or _is_num(v))\n",
    "        is_num_col[c] = bool(col.all())\n",
    "\n",
    "    # stringify data for width calc\n",
    "    data_str = df.map(_fmt_cell)\n",
    "\n",
    "    # compute widths (header vs content)\n",
    "    col_widths = {}\n",
    "    for c in df.columns:\n",
    "        header = str(c)\n",
    "        body_w = 0 if data_str.empty else data_str[c].map(len).max()\n",
    "        col_widths[c] = max(len(header), body_w)\n",
    "\n",
    "    idx_name = index_name if index_name is not None else (df.index.name or \"\")\n",
    "    idx_vals = df.index.astype(str)\n",
    "    idx_width = max(len(str(idx_name)), (0 if df.index.empty else idx_vals.map(len).max()))\n",
    "\n",
    "    def _hseg(left, mid, right):\n",
    "        # each cell prints as \" {content:<w} \" or \" {content:>w} \" so add 2\n",
    "        return left + H * (idx_width + 2) + \"\".join(\n",
    "            mid + H * (col_widths[c] + 2) for c in df.columns\n",
    "        ) + right\n",
    "\n",
    "    top    = _hseg(TL, TM, TR)\n",
    "    mid    = _hseg(ML, MM, MR)\n",
    "    bottom = _hseg(BL, BM, BR)\n",
    "\n",
    "    # header: index left; numeric headers right; text headers left\n",
    "    hdr_idx = f\"{idx_name:<{idx_width}}\"\n",
    "    hdr_cells = []\n",
    "    for c in df.columns:\n",
    "        h = str(c)\n",
    "        if is_num_col[c]:\n",
    "            hdr_cells.append(f\"{h:>{col_widths[c]}}\")\n",
    "        else:\n",
    "            hdr_cells.append(f\"{h:<{col_widths[c]}}\")\n",
    "    hdr_line = f\"{V} {hdr_idx} {V}\" + f\"{V}\".join(f\" {h} \" for h in hdr_cells) + f\"{V}\"\n",
    "\n",
    "    print(top)\n",
    "    print(hdr_line)\n",
    "    print(mid)\n",
    "\n",
    "    split_after = set(split_after_rows or [])\n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        idx_cell = f\"{str(idx):<{idx_width}}\"\n",
    "        body_cells = []\n",
    "        for c in df.columns:\n",
    "            s = _fmt_cell(row[c])\n",
    "            if is_num_col[c] and s != \"\":\n",
    "                body_cells.append(f\"{s:>{col_widths[c]}}\")\n",
    "            else:\n",
    "                body_cells.append(f\"{s:<{col_widths[c]}}\")\n",
    "        line = f\"{V} {idx_cell} {V}\" + f\"{V}\".join(f\" {cell} \" for cell in body_cells) + f\"{V}\"\n",
    "        print(line)\n",
    "        if i in split_after and i != len(df) - 1:\n",
    "            print(mid)\n",
    "\n",
    "    print(bottom)\n",
    "    print()\n",
    "\n",
    "\n",
    "def _print_pretty_classification_report(\n",
    "    y_true, y_pred, class_names=None, digits=4,\n",
    "    title=\"Classification Report (test set)\", ascii_borders=True\n",
    "):\n",
    "    rep = classification_report(y_true, y_pred, target_names=class_names,\n",
    "                                output_dict=True, zero_division=0)\n",
    "    order_cols = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "\n",
    "    # class rows\n",
    "    class_labels = class_names if class_names is not None else [\n",
    "        k for k in rep.keys() if k not in (\"accuracy\", \"macro avg\", \"weighted avg\")\n",
    "    ]\n",
    "    df_classes = pd.DataFrame(\n",
    "        [[rep[l].get(c, \"\") for c in order_cols] for l in class_labels],\n",
    "        index=class_labels, columns=order_cols\n",
    "    )\n",
    "\n",
    "    # --- FIXED: use empty strings instead of None/NaN in the accuracy row ---\n",
    "    total_support = int(sum(rep[l][\"support\"] for l in class_labels))\n",
    "    acc_row = pd.DataFrame(\n",
    "        [{\"precision\": \"\", \"recall\": \"\", \"f1-score\": rep[\"accuracy\"], \"support\": total_support}],\n",
    "        index=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    df_macro    = pd.DataFrame([[rep[\"macro avg\"][c]    for c in order_cols]], index=[\"macro avg\"],    columns=order_cols)\n",
    "    df_weighted = pd.DataFrame([[rep[\"weighted avg\"][c] for c in order_cols]], index=[\"weighted avg\"], columns=order_cols)\n",
    "\n",
    "    df = pd.concat([df_classes, acc_row, df_macro, df_weighted], axis=0)\n",
    "\n",
    "    # draw a horizontal rule after per-class rows\n",
    "    split_after = [len(df_classes) - 1]\n",
    "    _print_box_table(df, title=title, digits=digits, split_after_rows=split_after, ascii_borders=ascii_borders)\n",
    "\n",
    "\n",
    "def plot_roc_ovr_sklearn_style(y_train, y_test, y_proba, class_names=None,\n",
    "                               fig_kw=dict(figsize=(6, 6)),\n",
    "                               title=\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\"):\n",
    "    \"\"\"\n",
    "    Plot OvR ROC curves with the same look & feel as the scikit-learn example:\n",
    "    - per-class curves via RocCurveDisplay (aqua/darkorange/cornflowerblue cycling)\n",
    "    - micro-average: deeppink, dotted, lw=4\n",
    "    - macro-average: navy, dotted, lw=4\n",
    "    - chance level dashed black\n",
    "    \"\"\"\n",
    "    # Binarize\n",
    "    lb = LabelBinarizer().fit(y_train)\n",
    "    y_onehot_test = lb.transform(y_test)\n",
    "    if y_onehot_test.shape[1] == 1:\n",
    "        # binary edge case -> make it 2 columns\n",
    "        y_onehot_test = np.c_[1 - y_onehot_test, y_onehot_test]\n",
    "\n",
    "    n_classes = y_onehot_test.shape[1]\n",
    "    if class_names is None:\n",
    "        class_names = [f\"class {i}\" for i in range(n_classes)]\n",
    "\n",
    "    # Per-class ROC (to compute macro later)\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Micro-average\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Macro-average\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"], tpr[\"macro\"] = all_fpr, mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # ---- Plot (sklearn style) ----\n",
    "    fig, ax = plt.subplots(**fig_kw)\n",
    "\n",
    "    # micro\n",
    "    ax.plot(\n",
    "        fpr[\"micro\"], tpr[\"micro\"],\n",
    "        label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n",
    "        color=\"deeppink\", linestyle=\":\", linewidth=4,\n",
    "    )\n",
    "\n",
    "    # macro\n",
    "    ax.plot(\n",
    "        fpr[\"macro\"], tpr[\"macro\"],\n",
    "        label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "        color=\"navy\", linestyle=\":\", linewidth=4,\n",
    "    )\n",
    "\n",
    "    # per-class curves via RocCurveDisplay (cycle colors as in the example)\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "    for class_id, color in zip(range(n_classes), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_onehot_test[:, class_id],\n",
    "            y_proba[:, class_id],\n",
    "            name=f\"ROC curve for {class_names[class_id]}\",\n",
    "            ax=ax,\n",
    "            plot_chance_level=(class_id == n_classes - 1),  # dashed black diagonal once\n",
    "            despine=True,\n",
    "            **dict(color=color, linewidth=2)\n",
    "        )\n",
    "\n",
    "    ax.set(\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=title,\n",
    "        # xlim=(0.0, 1.0),\n",
    "        # ylim=(0.0, 1.05),\n",
    "    )\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "def plot_loss_per_epoch(model):\n",
    "    if isinstance(model, LogRegNumpy):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(16, 6), sharey='row')\n",
    "\n",
    "        ax[0].spines[['right', 'top']].set_visible(False)\n",
    "        ax[0].grid(ls='--', alpha=0.6)\n",
    "        ax[0].plot(model.loss_values[0], label='train')\n",
    "        ax[0].plot(model.loss_values[1], label='test')\n",
    "        ax[0].set_xlabel('Epoch')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].set_title(\"Train vs Test loss\")\n",
    "        ax[0].legend()\n",
    "\n",
    "        ax[1].spines[['right', 'top']].set_visible(False)\n",
    "        ax[1].grid(ls='--', alpha=0.6)\n",
    "        ax[1].plot(model.rec_history)\n",
    "        ax[1].set_xlabel('Step')\n",
    "        ax[1].set_ylabel('Loss (smoothed)')\n",
    "        ax[1].set_title(\"Train (smoothed) loss\")\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Model should be of type LogRegNumpy')\n",
    "    return\n",
    "\n",
    "# ----------------- One-model benchmark (same interface) -----------------\n",
    "\n",
    "def benchmark_classifier(\n",
    "    model_cls,\n",
    "    X_train, y_train,\n",
    "    X_test,  y_test,\n",
    "    *,\n",
    "    cv_folds=5,\n",
    "    model_args=None,\n",
    "    fit_args=None,\n",
    "    random_state=18092025,\n",
    "    class_names=None,\n",
    "    plot_loss=True,\n",
    "    plot_roc=True,\n",
    "    plot_confusions=True,\n",
    "    plot_margins=True,\n",
    "    ascii_borders=True,\n",
    "    eps=1.0\n",
    "):\n",
    "    model_args = model_args or {}\n",
    "    fit_args   = fit_args   or {}\n",
    "\n",
    "    # ===== 1) Simple train of Stratified CV on train =====\n",
    "    if cv_folds > 1:\n",
    "        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "        per_fold = []\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
    "            Xtr, Xva = X_train[tr_idx], X_train[va_idx]\n",
    "            ytr, yva = y_train[tr_idx], y_train[va_idx]\n",
    "\n",
    "            model = model_cls(**model_args)\n",
    "\n",
    "            # timing\n",
    "            t0 = perf_counter()\n",
    "            _try_fit(model, Xtr, ytr, fit_args, Xval=Xva, yval=yva)\n",
    "            fit_time = perf_counter() - t0\n",
    "\n",
    "            yhat = model.predict(Xva)\n",
    "\n",
    "            acc  = accuracy_score(yva, yhat)\n",
    "            p_micro, r_micro, f_micro, _ = precision_recall_fscore_support(yva, yhat, average=\"micro\",    zero_division=0)\n",
    "            p_macro, r_macro, f_macro, _ = precision_recall_fscore_support(yva, yhat, average=\"macro\",    zero_division=0)\n",
    "            p_w, r_w, f_w, _             = precision_recall_fscore_support(yva, yhat, average=\"weighted\", zero_division=0)\n",
    "\n",
    "            per_fold.append(dict(\n",
    "                fold=fold,\n",
    "                accuracy=acc,\n",
    "                precision_micro=p_micro, recall_micro=r_micro, f1_micro=f_micro,\n",
    "                precision_macro=p_macro, recall_macro=r_macro, f1_macro=f_macro,\n",
    "                precision_weighted=p_w, recall_weighted=r_w, f1_weighted=f_w,\n",
    "                fit_time_sec=fit_time,\n",
    "            ))\n",
    "\n",
    "        cv_df = pd.DataFrame(per_fold).set_index(\"fold\")\n",
    "        _print_cv_tables(cv_df, title=\"Cross-Validation (per-fold + mean)\", ascii_borders=ascii_borders)\n",
    "\n",
    "    # ===== 2) Fit on full train, evaluate on test =====\n",
    "    final_model = model_cls(**model_args)\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    _try_fit(final_model, X_train, y_train, fit_args)\n",
    "    final_fit_time = perf_counter() - t0\n",
    "\n",
    "    y_pred  = final_model.predict(X_test)\n",
    "    y_proba = _proba(final_model, X_test)\n",
    "\n",
    "    _print_pretty_classification_report(\n",
    "        y_test, y_pred, class_names=class_names, digits=4,\n",
    "        title=f\"Classification Report (test set)  [final fit {final_fit_time:.2f}s]\",\n",
    "        ascii_borders=ascii_borders\n",
    "    )\n",
    "    \n",
    "    # ===== 3) Plots (Loss + ROC + Confusions + Margins) =====\n",
    "    if plot_loss:\n",
    "        plot_loss_per_epoch(final_model)\n",
    "\n",
    "    if plot_roc:\n",
    "        plot_roc_ovr_sklearn_style(y_train, y_test, y_proba, class_names=class_names)\n",
    "\n",
    "    if plot_confusions:\n",
    "        plot_multiclass_confusion_matrix(\n",
    "            y_test, y_pred, class_names=class_names, normalize=True,\n",
    "            title=\"Confusion Matrix (test, row-normalized)\"\n",
    "        )\n",
    "        plot_tp_fp_fn_tn_table(\n",
    "            y_test, y_pred, class_names=class_names,\n",
    "            title=\"Per-class TP/FP/FN/TN (test)\"\n",
    "        )\n",
    "\n",
    "    # Optional margin curves (only if model implements `calc_margins`)\n",
    "    if plot_margins and hasattr(final_model, \"calc_margins\") and callable(getattr(final_model, \"calc_margins\")):\n",
    "        try:\n",
    "            # Train margins (on full train the model was fit on)\n",
    "            train_margins = final_model.calc_margins(X_train, y_train)\n",
    "            viz_margins(train_margins, eps=eps)\n",
    "            # Test margins\n",
    "            test_margins  = final_model.calc_margins(X_test,  y_test)\n",
    "            viz_margins(test_margins, eps=eps)\n",
    "        except Exception as e:\n",
    "            # Be silent but informative\n",
    "            print(f\"[info] Skipped margin plots due to error: {e}\")\n",
    "\n",
    "    # ===== 4) Return results (kept compatible) =====\n",
    "    results = {\n",
    "        \"cv_per_fold\": cv_df,                              # per-fold metrics (+ fit_time_sec)\n",
    "        \"cv_mean\": cv_df.mean(numeric_only=True).to_frame().T,  # one-row DataFrame\n",
    "        \"y_test\": y_test,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"final_fit_time_sec\": final_fit_time,              # extra convenience\n",
    "    }\n",
    "    return final_model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de848183",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    # --- base params\n",
    "    init_strategy = 'normal',\n",
    "    # --- fit args\n",
    "    total_steps = X_train_scaled.shape[0]*100, \n",
    "    learning_rate = 1e-2,\n",
    "    gd_algo = 'sgd',\n",
    "    batch_size = 1,\n",
    "    momentum = 0.0,\n",
    "    l2 = 0.01, \n",
    "    optim_step = False,\n",
    "    # early stopping\n",
    "    early_stopping = True,\n",
    "    tolerance = 1e-4,\n",
    "    n_startup_rounds = 0,\n",
    "    early_stop_rounds = 5,\n",
    "    validation_fraction = 0.1,\n",
    "    # рекуррентная оценка функции потерь\n",
    "    rec_mode = 'ema', \n",
    "    ema_lambda = 0.01,\n",
    "    # стратегия сэмплирования\n",
    "    sampling_mode = 'uniform',\n",
    "    shuffle = True, \n",
    "    sampling_tau = 0.5,\n",
    "    sampling_min_prob = 0.01, \n",
    "    refresh_rate = 200,\n",
    "    # --- logs\n",
    "    steps_per_epoch = 500,\n",
    "    verbose=False,\n",
    "    # --- misc\n",
    "    use_best_weights = False,\n",
    "    return_weights_history = False,\n",
    "    random_seed = SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39139111",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model, results_logreg = benchmark_classifier(\n",
    "    LogRegNumpy,\n",
    "    X_train_scaled, y_train_enc,\n",
    "    X_test_scaled, y_test_enc,\n",
    "    cv_folds=5,\n",
    "    model_args=model_args,\n",
    "    class_names=label_encoder.classes_,\n",
    "    plot_loss=True,\n",
    "    plot_roc=True,\n",
    "    plot_confusions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b646bb8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarginMixin:\n",
    "    def calc_margins(self, X: np.ndarray, y_true: np.ndarray):\n",
    "        logits = self.decision_function(X)\n",
    "        # gather true-class scores\n",
    "        true_logits = logits[np.arange(X.shape[0]), y_true]\n",
    "        # mask out the true class to get the best 'false' score per row\n",
    "        logits[np.arange(logits.shape[0]), y_true] = -np.inf\n",
    "        false_logits = logits.max(axis=1)\n",
    "        margins = true_logits - false_logits\n",
    "        return margins\n",
    "    \n",
    "def with_margins(BaseCls):\n",
    "    bases = (BaseCls, MarginMixin) if issubclass(BaseCls, BaseEstimator) else (BaseCls, MarginMixin, BaseEstimator)\n",
    "    name = f\"{BaseCls.__name__}SK\"\n",
    "    cls = type(name, bases, {}) # metaclass?\n",
    "    cls.__module__ = BaseCls.__module__\n",
    "    cls.__doc__ = (BaseCls.__doc__ or \"\") + (\n",
    "        \"\\n\\nThis subclass adds `calc_margins(X, y_true)` to compute per-sample \"\n",
    "        \"margins = score(true) - max_{c!=true} score(c).\"\n",
    "    )\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2feb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDClassifierSK = with_margins(SGDClassifier)\n",
    "LogisticRegressionSK = with_margins(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93799f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_logreg_sk = SGDClassifierSK(\n",
    "    loss = 'log_loss',\n",
    "    penalty='l2',\n",
    "    alpha=0.0,\n",
    "    max_iter=1000,\n",
    "    tol=1e-4,\n",
    "    shuffle=True,\n",
    "    random_state=SEED,\n",
    "    learning_rate='constant',\n",
    "    eta0=1e-2,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=50,\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbaf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_logreg_sk.fit(X_train_scaled, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sk = sgd_logreg_sk.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test accuracy: {(preds_sk == y_test_enc).mean()*100.:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ccd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "margins_train = sgd_logreg_sk.calc_margins(X_train_scaled, y_train_enc)\n",
    "margins_test  = sgd_logreg_sk.calc_margins(X_test_scaled,  y_test_enc)\n",
    "\n",
    "viz_margins(margins_train)\n",
    "viz_margins(margins_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    loss = 'log_loss',\n",
    "    penalty = 'l2',\n",
    "    alpha = 0.0,\n",
    "    max_iter = 1000,\n",
    "    tol = 1e-4,\n",
    "    shuffle = True,\n",
    "    random_state = SEED,\n",
    "    learning_rate = 'constant',\n",
    "    eta0 = 1e-2,\n",
    "    early_stopping = False,\n",
    "    validation_fraction = 0.1,\n",
    "    n_iter_no_change = 50,\n",
    "    n_jobs = 1,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model, results_sgdsk = benchmark_classifier(\n",
    "    SGDClassifierSK,\n",
    "    X_train_scaled, y_train_enc,\n",
    "    X_test_scaled, y_test_enc,\n",
    "    cv_folds=5,\n",
    "    model_args=model_args,\n",
    "    class_names=label_encoder.classes_,\n",
    "    plot_roc=True,\n",
    "    plot_confusions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc059b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_sk = LogisticRegressionSK(\n",
    "    penalty=None, #'l2'\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    tol=1e-4,\n",
    "    random_state=SEED,\n",
    "    solver='lbfgs',\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_sk.fit(X_train_scaled, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sk = logreg_sk.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d571c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test accuracy: {(preds_sk == y_test_enc).mean()*100.:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ff778",
   "metadata": {},
   "outputs": [],
   "source": [
    "margins_train = logreg_sk.calc_margins(X_train_scaled, y_train_enc)\n",
    "margins_test  = logreg_sk.calc_margins(X_test_scaled,  y_test_enc)\n",
    "\n",
    "viz_margins(margins_train)\n",
    "viz_margins(margins_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b54eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    penalty = None, #'l2'\n",
    "    C = 1.0,\n",
    "    max_iter = 1000,\n",
    "    tol = 1e-4,\n",
    "    random_state = SEED,\n",
    "    solver = 'lbfgs',\n",
    "    n_jobs = 1,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model, results_logregsk = benchmark_classifier(\n",
    "    LogisticRegressionSK,\n",
    "    X_train_scaled, y_train_enc,\n",
    "    X_test_scaled, y_test_enc,\n",
    "    cv_folds=5,\n",
    "    model_args=model_args,\n",
    "    class_names=label_encoder.classes_,\n",
    "    plot_roc=True,\n",
    "    plot_confusions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd959b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = pd.concat([results_logreg['cv_mean'], results_sgdsk['cv_mean'], results_logregsk['cv_mean']], axis=0)\n",
    "comp.index = ['LogRegNumpy', 'SGDClassifierSK', 'LogisticRegressionSK']\n",
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a995925",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dict = {\n",
    "    'LogRegNumpy': results_logreg['cv_mean'].values.ravel(),\n",
    "    'SGDRRegressor': results_sgdsk['cv_mean'].values.ravel(),\n",
    "    'LogisticRegression': results_logregsk['cv_mean'].values.ravel(),\n",
    "}\n",
    "cols = results_logreg['cv_mean'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame.from_dict(comp_dict, orient='index', columns=cols)\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf07f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl python 3.11.5",
   "language": "python",
   "name": "wsl_main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
